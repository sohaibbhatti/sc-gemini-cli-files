143
for your learning problem. For example, if you are using Bayesian logistic re-
gression, then you might choose p(y(i)|x(i), θ) = hθ(x(i))y(i)
(1−hθ(x(i)))(1−y(i)),
where hθ(x(i)) = 1/(1 + exp(−θT x(i))).7
When we are given a new test example x and asked to make it prediction
on it, we can compute our posterior distribution on the class label using the
posterior distribution on θ:
p(y|x, S) =
∫
θ
p(y|x, θ)p(θ|S)dθ (9.4)
In the equation above, p(θ|S) comes from Equation (9.3). Thus, for example,
if the goal is to the predict the expected value of y given x, then we would
output8
E[y|x, S] =
∫
y
yp(y|x, S)dy
The procedure that we’ve outlined here can be thought of as doing “fully
Bayesian” prediction, where our prediction is computed by taking an average
with respect to the posterior p(θ|S) over θ. Unfortunately, in general it is
computationally very diﬃcult to compute this posterior distribution. This is
because it requires taking integrals over the (usually high-dimensional) θ as
in Equation (9.3), and this typically cannot be done in closed-form.
Thus, in practice we will instead approximate the posterior distribution
for θ. One common approximation is to replace our posterior distribution for
θ (as in Equation 9.4) with a single point estimate. The MAP (maximum
a posteriori) estimate for θ is given by
θMAP = arg max
θ
n∏
i=1
p(y(i)|x(i), θ)p(θ). (9.5)
Note that this is the same formulas as for the MLE (maximum likelihood)
estimate for θ, except for the prior p(θ) term at the end.
In practical applications, a common choice for the prior p(θ) is to assume
that θ ∼ N (0, τ2I). Using this choice of prior, the ﬁtted parametersθMAP will
have smaller norm than that selected by maximum likelihood. In practice,
this causes the Bayesian MAP estimate to be less susceptible to overﬁtting
than the ML estimate of the parameters. For example, Bayesian logistic
regression turns out to be an eﬀective algorithm for text classiﬁcation, even
though in text classiﬁcation we usually have d ≫ n.
7Since we are now viewing θ as a random variable, it is okay to condition on it value,
and write “p(y|x,θ )” instead of “p(y|x;θ).”
8The integral below would be replaced by a summation if y is discrete-valued.

Part IV
Unsupervised learning
144

Chapter 10
Clustering and the k-means
algorithm
In the clustering problem, we are given a training set {x(1), . . . , x(n)}, and
want to group the data into a few cohesive “clusters.” Here, x(i) ∈ Rd
as usual; but no labels y(i) are given. So, this is an unsupervised learning
problem.
The k-means clustering algorithm is as follows:
1. Initialize cluster centroids µ1, µ2, . . . , µk ∈ Rd randomly.
2. Repeat until convergence: {
For every i, set
c(i) := arg min
j
||x(i) − µj||2.
For each j, set
µj :=
∑n
i=1 1{c(i) = j}x(i)
∑n
i=1 1{c(i) = j} .
}
In the algorithm above, k (a parameter of the algorithm) is the number
of clusters we want to ﬁnd; and the cluster centroids µj represent our current
guesses for the positions of the centers of the clusters. To initialize the cluster
centroids (in step 1 of the algorithm above), we could choose k training
examples randomly, and set the cluster centroids to be equal to the values of
these k examples. (Other initialization methods are also possible.)
The inner-loop of the algorithm repeatedly carries out two steps: (i)
“Assigning” each training example x(i) to the closest cluster centroid µj, and
145

146
Figure 10.1: K-means algorithm. Training examples are shown as dots, and
cluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-
tial cluster centroids (in this instance, not chosen to be equal to two training
examples). (c-f) Illustration of running two iterations of k-means. In each
iteration, we assign each training example to the closest cluster centroid
(shown by “painting” the training examples the same color as the cluster
centroid to which is assigned); then we move each cluster centroid to the
mean of the points assigned to it. (Best viewed in color.) Images courtesy
Michael Jordan.
(ii) Moving each cluster centroid µj to the mean of the points assigned to it.
Figure 10.1 shows an illustration of running k-means.
Is the k-means algorithm guaranteed to converge? Yes it is, in a certain
sense. In particular, let us deﬁne the distortion function to be:
J(c, µ) =
n∑
i=1
||x(i) − µc(i)||2
Thus, J measures the sum of squared distances between each training exam-
ple x(i) and the cluster centroid µc(i) to which it has been assigned. It can
be shown that k-means is exactly coordinate descent on J. Speciﬁcally, the
inner-loop of k-means repeatedly minimizes J with respect to c while holding
µ ﬁxed, and then minimizes J with respect to µ while holding c ﬁxed. Thus,