28
Lastly, in our logistic regression setting, θ is vector-valued, so we need to
generalize Newton’s method to this setting. The generalization of Newton’s
method to this multidimensional setting (also called the Newton-Raphson
method) is given by
θ := θ − H−1∇θℓ(θ).
Here, ∇θℓ(θ) is, as usual, the vector of partial derivatives of ℓ(θ) with respect
to the θi’s; and H is an d-by-d matrix (actually, d+1−by−d+1, assuming that
we include the intercept term) called the Hessian, whose entries are given
by
Hij = ∂2ℓ(θ)
∂θi∂θj
.
Newton’s method typically enjoys faster convergence than (batch) gra-
dient descent, and requires many fewer iterations to get very close to the
minimum. One iteration of Newton’s can, however, be more expensive than
one iteration of gradient descent, since it requires ﬁnding and inverting an
d-by-d Hessian; but so long as d is not too large, it is usually much faster
overall. When Newton’s method is applied to maximize the logistic regres-
sion log likelihood function ℓ(θ), the resulting method is also called Fisher
scoring.

Chapter 3
Generalized linear models
So far, we’ve seen a regression example, and a classiﬁcation example. In the
regression example, we had y|x; θ ∼ N (µ, σ2), and in the classiﬁcation one,
y|x; θ ∼ Bernoulli(φ), for some appropriate deﬁnitions of µ and φ as functions
of x and θ. In this section, we will show that both of these methods are
special cases of a broader family of models, called Generalized Linear Models
(GLMs).1 We will also show how other models in the GLM family can be
derived and applied to other classiﬁcation and regression problems.
3.1 The exponential family
To work our way up to GLMs, we will begin by deﬁning exponential family
distributions. We say that a class of distributions is in the exponential family
if it can be written in the form
p(y; η) = b(y) exp(ηT T (y) − a(η)) (3.1)
Here, η is called the natural parameter (also called the canonical param-
eter) of the distribution; T (y) is the suﬃcient statistic (for the distribu-
tions we consider, it will often be the case that T (y) = y); and a(η) is the log
partition function. The quantity e−a(η) essentially plays the role of a nor-
malization constant, that makes sure the distribution p(y; η) sums/integrates
over y to 1.
A ﬁxed choice of T , a and b deﬁnes a family (or set) of distributions that
is parameterized by η; as we vary η, we then get diﬀerent distributions within
this family.
1The presentation of the material in this section takes inspiration from Michael I.
Jordan, Learning in graphical models (unpublished book draft), and also McCullagh and
Nelder, Generalized Linear Models (2nd ed.) .
29

30
We now show that the Bernoulli and the Gaussian distributions are ex-
amples of exponential family distributions. The Bernoulli distribution with
mean φ, written Bernoulli(φ), speciﬁes a distribution over y ∈ {0, 1}, so that
p(y = 1; φ) = φ; p(y = 0; φ) = 1 − φ. As we vary φ, we obtain Bernoulli
distributions with diﬀerent means. We now show that this class of Bernoulli
distributions, ones obtained by varying φ, is in the exponential family; i.e.,
that there is a choice of T , a and b so that Equation (3.1) becomes exactly
the class of Bernoulli distributions.
We write the Bernoulli distribution as:
p(y; φ) = φy(1 − φ)1−y
= exp( y log φ + (1 − y) log(1 − φ))
= exp
((
log
( φ
1 − φ
))
y + log(1 − φ)
)
.
Thus, the natural parameter is given by η = log(φ/(1 − φ)). Interestingly, if
we invert this deﬁnition for η by solving for φ in terms of η, we obtain φ =
1/(1 + e−η). This is the familiar sigmoid function! This will come up again
when we derive logistic regression as a GLM. To complete the formulation
of the Bernoulli distribution as an exponential family distribution, we also
have
T (y) = y
a(η) = − log(1 − φ)
= log(1 + eη)
b(y) = 1
This shows that the Bernoulli distribution can be written in the form of
Equation (3.1), using an appropriate choice of T , a and b.
Let’s now move on to consider the Gaussian distribution. Recall that,
when deriving linear regression, the value of σ2 had no eﬀect on our ﬁnal
choice of θ and hθ(x). Thus, we can choose an arbitrary value for σ2 without
changing anything. To simplify the derivation below, let’s set σ2 = 1.2 We
2If we leave σ2 as a variable, the Gaussian distribution can also be shown to be in the
exponential family, whereη ∈ R2 is now a 2-dimension vector that depends on both µ and
σ. For the purposes of GLMs, however, theσ2 parameter can also be treated by considering
a more general deﬁnition of the exponential family: p(y;η,τ ) = b(a,τ ) exp((ηTT (y) −
a(η))/c(τ)). Here, τ is called the dispersion parameter, and for the Gaussian,c(τ) =σ2;
but given our simpliﬁcation above, we won’t need the more general deﬁnition for the
examples we will consider here.

31
then have:
p(y; µ) = 1√
2π exp
(
−1
2(y − µ)2
)
= 1√
2π exp
(
−1
2 y2
)
· exp
(
µy − 1
2 µ2
)
Thus, we see that the Gaussian is in the exponential family, with
η = µ
T (y) = y
a(η) = µ2/2
= η2/2
b(y) = (1 /
√
2π) exp(−y2/2).
There’re many other distributions that are members of the exponen-
tial family: The multinomial (which we’ll see later), the Poisson (for mod-
elling count-data; also see the problem set); the gamma and the exponen-
tial (for modelling continuous, non-negative random variables, such as time-
intervals); the beta and the Dirichlet (for distributions over probabilities);
and many more. In the next section, we will describe a general “recipe”
for constructing models in which y (given x and θ) comes from any of these
distributions.
3.2 Constructing GLMs
Suppose you would like to build a model to estimate the number y of cus-
tomers arriving in your store (or number of page-views on your website) in
any given hour, based on certain features x such as store promotions, recent
advertising, weather, day-of-week, etc. We know that the Poisson distribu-
tion usually gives a good model for numbers of visitors. Knowing this, how
can we come up with a model for our problem? Fortunately, the Poisson is an
exponential family distribution, so we can apply a Generalized Linear Model
(GLM). In this section, we will we will describe a method for constructing
GLM models for problems such as these.
More generally, consider a classiﬁcation or regression problem where we
would like to predict the value of some random variable y as a function of
x. To derive a GLM for this problem, we will make the following three
assumptions about the conditional distribution of y given x and about our
model:

32
1. y | x; θ ∼ ExponentialFamily(η). I.e., given x and θ, the distribution of
y follows some exponential family distribution, with parameter η.
2. Given x, our goal is to predict the expected value of T (y) given x.
In most of our examples, we will have T (y) = y, so this means we
would like the prediction h(x) output by our learned hypothesis h to
satisfy h(x) = E[ y|x]. (Note that this assumption is satisﬁed in the
choices for hθ(x) for both logistic regression and linear regression. For
instance, in logistic regression, we had hθ(x) = p(y = 1|x; θ) = 0 · p(y =
0|x; θ) + 1 · p(y = 1|x; θ) = E[y|x; θ].)
3. The natural parameter η and the inputs x are related linearly: η = θT x.
(Or, if η is vector-valued, then ηi = θT
i x.)
The third of these assumptions might seem the least well justiﬁed of
the above, and it might be better thought of as a “design choice” in our
recipe for designing GLMs, rather than as an assumption per se. These
three assumptions/design choices will allow us to derive a very elegant class
of learning algorithms, namely GLMs, that have many desirable properties
such as ease of learning. Furthermore, the resulting models are often very
eﬀective for modelling diﬀerent types of distributions over y; for example, we
will shortly show that both logistic regression and ordinary least squares can
both be derived as GLMs.
3.2.1 Ordinary least squares
To show that ordinary least squares is a special case of the GLM family
of models, consider the setting where the target variable y (also called the
response variable in GLM terminology) is continuous, and we model the
conditional distribution of y given x as a Gaussian N (µ, σ2). (Here, µ may
depend x.) So, we let the ExponentialF amily(η) distribution above be
the Gaussian distribution. As we saw previously, in the formulation of the
Gaussian as an exponential family distribution, we had µ = η. So, we have
hθ(x) = E[y|x; θ]
= µ
= η
= θT x.
The ﬁrst equality follows from Assumption 2, above; the second equality
follows from the fact thaty|x; θ ∼ N (µ, σ2), and so its expected value is given