219
We aim to use gradient ascent to maximize η(θ). The main challenge
we face here is to compute (or estimate) the gradient of η(θ) without the
knowledge of the form of the reward function and the transition probabilities.
Let Pθ(τ) denote the distribution of τ (generated by the policy πθ), and
let f(τ) = ∑T−1
t=0 γtR(st, at). We can rewrite η(θ) as
η(θ) = Eτ∼Pθ [f(τ)] (17.2)
We face a similar situations in the variational auto-encoder (VAE) setting
covered in the previous lectures, where the we need to take the gradient w.r.t
to a variable that shows up under the expectation — the distribution Pθ
depends on θ. Recall that in VAE, we used the re-parametrization techniques
to address this problem. However it does not apply here because we do
know not how to compute the gradient of the function f. (We only have
an eﬃcient way to evaluate the function f by taking a weighted sum of the
observed rewards, but we do not necessarily know the reward function itself
to compute the gradient.)
The REINFORCE algorithm uses an another approach to estimate the
gradient of η(θ). We start with the following derivation:
∇θEτ∼Pθ [f(τ)] = ∇θ
∫
Pθ(τ)f(τ)dτ
=
∫
∇θ(Pθ(τ)f(τ))dτ (swap integration with gradient)
=
∫
(∇θPθ(τ))f(τ)dτ (becaue f does not depend on θ)
=
∫
Pθ(τ)(∇θ log Pθ(τ))f(τ)dτ
(because ∇ log Pθ(τ) = ∇Pθ(τ)
Pθ(τ) )
= Eτ∼Pθ [(∇θ log Pθ(τ))f(τ)] (17.3)
Now we have a sample-based estimator for ∇θEτ∼Pθ [f(τ)]. Let τ (1), . . . , τ(n)
be n empirical samples from Pθ (which are obtained by running the policy
πθ for n times, with T steps for each run). We can estimate the gradient of
η(θ) by
∇θEτ∼Pθ [f(τ)] = Eτ∼Pθ [(∇θ log Pθ(τ))f(τ)] (17.4)
≈ 1
n
n∑
i=1
(∇θ log Pθ(τ (i)))f(τ (i)) (17.5)

220
The next question is how to compute log Pθ(τ). We derive an analyt-
ical formula for log Pθ(τ) and compute its gradient w.r.t θ (using auto-
diﬀerentiation). Using the deﬁnition of τ, we have
Pθ(τ) = µ(s0)πθ(a0|s0)Ps0a0(s1)πθ(a1|s1)Ps1a1(s2) · · · PsT−1aT−1(sT ) (17.6)
Here recall that µ to used to denote the density of the distribution of s0. It
follows that
log Pθ(τ) = log µ(s0) + logπθ(a0|s0) + logPs0a0(s1) + logπθ(a1|s1)
+ log Ps1a1(s2) + · · · + log PsT−1aT−1(sT ) (17.7)
Taking gradient w.r.t to θ, we obtain
∇θ log Pθ(τ) = ∇θ log πθ(a0|s0) + ∇θ log πθ(a1|s1) + · · · + ∇θ log πθ(aT−1|sT−1)
Note that many of the terms disappear because they don’t depend on θ and
thus have zero gradients. (This is somewhat important — we don’t know how
to evaluate those terms such as log Ps0a0(s1) because we don’t have access to
the transition probabilities, but luckily those terms have zero gradients!)
Plugging the equation above into equation (17.4), we conclude that
∇θη(θ) = ∇θEτ∼Pθ [f(τ)] = Eτ∼Pθ
[( T−1∑
t=0
∇θ log πθ(at|st)
)
· f(τ)
]
= Eτ∼Pθ
[( T−1∑
t=0
∇θ log πθ(at|st)
)
·
(T−1∑
t=0
γtR(st, at)
)]
(17.8)
We estimate the RHS of the equation above by empirical sample trajectories,
and the estimate is unbiased. The vanilla REINFORCE algorithm iteratively
updates the parameter by gradient ascent using the estimated gradients.
Interpretation of the policy gradient formula (17.8). The quantity
∇θPθ(τ) = ∑T−1
t=0 ∇θ log πθ(at|st) is intuitively the direction of the change
of θ that will make the trajectory τ more likely to occur (or increase the
probability of choosing action a0, . . . , at−1), and f(τ) is the total payoﬀ of
this trajectory. Thus, by taking a gradient step, intuitively we are trying to
improve the likelihood of all the trajectories, but with a diﬀerent emphasis
or weight for each τ (or for each set of actions a0, a1, . . . , at−1). If τ is very
rewarding (that is, f(τ) is large), we try very hard to move in the direction

221
that can increase the probability of the trajectory τ (or the direction that
increases the probability of choosing a0, . . . , at−1), and if τ has low payoﬀ,
we try less hard with a smaller weight.
An interesting fact that follows from formula (17.3) is that
Eτ∼Pθ
[ T−1∑
t=0
∇θ log πθ(at|st)
]
= 0 (17.9)
To see this, we take f(τ) = 1 (that is, the reward is always a constant),
then the LHS of (17.8) is zero because the payoﬀ is always a ﬁxed constant∑T
t=0 γt. Thus the RHS of (17.8) is also zero, which implies (17.9).
In fact, one can verify that E at∼πθ(·|st)∇θ log πθ(at|st) = 0 for any ﬁxed t
and st.2 This fact has two consequences. First, we can simplify formula (17.8)
to
∇θη(θ) =
T−1∑
t=0
Eτ∼Pθ
[
∇θ log πθ(at|st) ·
(T−1∑
j=0
γjR(sj, aj)
)]
=
T−1∑
t=0
Eτ∼Pθ
[
∇θ log πθ(at|st) ·
(T−1∑
j≥t
γjR(sj, aj)
)]
(17.10)
where the second equality follows from
Eτ∼Pθ
[
∇θ log πθ(at|st) ·
( ∑
0≤j<t
γjR(sj, aj)
)]
= E
[
E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] ·
( ∑
0≤j<t
γjR(sj, aj)
)]
= 0 (because E [ ∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] = 0)
Note that here we used the law of total expectation. The outer expecta-
tion in the second line above is over the randomness of s0, a0, . . . , at−1, st,
whereas the inner expectation is over the randomness of at (conditioned on
s0, a0, . . . , at−1, st.) We see that we’ve made the estimator slightly simpler.
The second consequence of Eat∼πθ(·|st)∇θ log πθ(at|st) = 0 is the following: for
any value B(st) that only depends on st, it holds that
Eτ∼Pθ [∇θ log πθ(at|st) · B(st)]
= E [E [∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] B(st)]
= 0 (because E [ ∇θ log πθ(at|st)|s0, a0, . . . , st−1, at−1, st] = 0)
2In general, it’s true that E x∼pθ[∇ logpθ(x)] = 0.

222
Again here we used the law of total expectation. The outer expecta-
tion in the second line above is over the randomness of s0, a0, . . . , at−1, st,
whereas the inner expectation is over the randomness of at (conditioned on
s0, a0, . . . , at−1, st.) It follows from equation (17.10) and the equation above
that
∇θη(θ) =
T−1∑
t=0
Eτ∼Pθ
[
∇θ log πθ(at|st) ·
(T−1∑
j≥t
γjR(sj, aj) − γtB(st)
)]
=
T−1∑
t=0
Eτ∼Pθ
[
∇θ log πθ(at|st) · γt
(T−1∑
j≥t
γj−tR(sj, aj) − B(st)
)]
(17.11)
Therefore, we will get a diﬀerent estimator for estimating the ∇η(θ) with a
diﬀerence choice of B(·). The beneﬁt of introducing a proper B(·) — which
is often referred to as a baseline — is that it helps reduce the variance of the
estimator.3 It turns out that a near optimal estimator would be the expected
future payoﬀ E
[∑T−1
j≥t γj−tR(sj, aj)|st
]
, which is pretty much the same as the
value function V πθ(st) (if we ignore the diﬀerence between ﬁnite and inﬁnite
horizon.) Here one could estimate the value function V πθ(·) in a crude way,
because its precise value doesn’t inﬂuence the mean of the estimator but only
the variance. This leads to a policy gradient algorithm with baselines stated
in Algorithm 7. 4
3As a heuristic but illustrating example, suppose for a ﬁxed t, the future reward∑T−1
j≥t γj−tR(sj,aj) randomly takes two values 1000 + 1 and 1000 − 2 with equal proba-
bility, and the corresponding values for ∇θ logπθ(at|st) are vector z and −z. (Note that
because E [∇θ logπθ(at|st)] = 0, if ∇θ logπθ(at|st) can only take two values uniformly,
then the two values have to two vectors in an opposite direction.) In this case, without
subtracting the baseline, the estimators take two values (1000 + 1) z and −(1000 − 2)z,
whereas after subtracting a baseline of 1000, the estimator has two values z and 2z. The
latter estimator has much lower variance compared to the original estimator.
4We note that the estimator of the gradient in the algorithm does not exactly match
the equation 17.11. If we multiply γt in the summand of equation (17.13), then they will
exactly match. Removing such discount factors empirically works well because it gives a
large update.

223
Algorithm 7 Vanilla policy gradient with baseline
for i = 1, · · · do
Collect a set of trajectories by executing the current policy. Use R≥t
as a shorthand for ∑T−1
j≥t γj−tR(sj, aj)
Fit the baseline by ﬁnding a function B that minimizes
∑
τ
∑
t
(R≥t − B(st))2 (17.12)
Update the policy parameter θ with the gradient estimator
∑
τ
∑
t
∇θ log πθ(at|st) · (R≥t − B(st)) (17.13)

Bibliography
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling
modern machine-learning practice and the classical bias–variance trade-
oﬀ. Proceedings of the National Academy of Sciences, 116(32):15849–15854,
2019.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for
weak features. SIAM Journal on Mathematics of Data Science , 2(4):1167–
1180, 2020.
David M Blei, Alp Kucukelbir, and Jon D McAuliﬀe. Variational inference:
A review for statisticians. Journal of the American Statistical Association ,
112(518):859–877, 2017.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran
Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine
Bosselut, Emma Brunskill, et al. On the opportunities and risks of foun-
dation models. arXiv preprint arXiv:2108.07258 , 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-
plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-
try, Amanda Askell, et al. Language models are few-shot learners.Advances
in neural information processing systems , 33:1877–1901, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton.
A simple framework for contrastive learning of visual representations. In
International Conference on Machine Learning , pages 1597–1607. PMLR,
2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
Pre-training of deep bidirectional transformers for language understand-
ing. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019.
224

225
Jeﬀ Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters:
Understanding the implicit bias of the noise covariance. arXiv preprint
arXiv:2006.08680, 2020.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-dimensional ridgeless least squares interpolation. 2019.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.
Surprises in high-dimensional ridgeless least squares interpolation. The
Annals of Statistics , 50(2):949–986, 2022.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–778, 2016.
Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An
introduction to statistical learning, second edition , volume 112. Springer,
2021.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 , 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes.arXiv
preprint arXiv:1312.6114, 2013.
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and
Tengyu Ma. Algorithmic framework for model-based deep reinforcement
learning with theoretical guarantees. InInternational Conference on Learn-
ing Representations, 2018.
Song Mei and Andrea Montanari. The generalization error of random features
regression: Precise asymptotics and the double descent curve. Communi-
cations on Pure and Applied Mathematics , 75(4):667–766, 2022.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise
double descent. 2019.
Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal
regularization can mitigate double descent. 2020.
Manfred Opper. Statistical mechanics of learning: Generalization. The hand-
book of brain theory and neural networks , pages 922–925, 1995.
Manfred Opper. Learning to generalize. Frontiers of Life, 3(part 2):763–775,
2001.

226
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you
need. arXiv preprint arXiv:1706.03762 , 2017.
Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-
dro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and
rich regimes in overparametrized models. arXiv preprint arXiv:2002.09277,
2020.