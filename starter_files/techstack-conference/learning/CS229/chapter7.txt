78
[L, H] interval, to get
Î±new
2 =
ï£±
ï£²
ï£³
H if Î±new,unclipped
2 > H
Î±new,unclipped
2 if L â‰¤ Î±new,unclipped
2 â‰¤ H
L if Î±new,unclipped
2 < L
Finally, having found the Î±new
2 , we can use Equation (6.22) to go back and
ï¬nd the optimal value of Î±new
1 .
Thereâ€™re a couple more details that are quite easy but that weâ€™ll leave you
to read about yourself in Plattâ€™s paper: One is the choice of the heuristics
used to select the next Î±i, Î±j to update; the other is how to update b as the
SMO algorithm is run.

Part II
Deep learning
79

Chapter 7
Deep learning
We now begin our study of deep learning. In this set of notes, we give an
overview of neural networks, discuss vectorization and discuss training neural
networks with backpropagation.
7.1 Supervised learning with non-linear mod-
els
In the supervised learning setting (predicting y from the input x), suppose
our model/hypothesis is hÎ¸(x). In the past lectures, we have considered the
cases when hÎ¸(x) = Î¸âŠ¤x (in linear regression) or hÎ¸(x) = Î¸âŠ¤Ï†(x) (where Ï†(x)
is the feature map). A commonality of these two models is that they are
linear in the parameters Î¸. Next we will consider learning general family of
models that are non-linear in both the parameters Î¸ and the inputs x. The
most common non-linear models are neural networks, which we will deï¬ne
staring from the next section. For this section, it suï¬ƒces to think hÎ¸(x) as
an abstract non-linear model. 1
Suppose {(x(i), y(i))}n
i=1 are the training examples. We will deï¬ne the
nonlinear model and the loss/cost function for learning it.
Regression problems. For simplicity, we start with the case where the
output is a real number, that is, y(i) âˆˆ R, and thus the model hÎ¸ also outputs
a real number hÎ¸(x) âˆˆ R. We deï¬ne the least square cost function for the
1If a concrete example is helpful, perhaps think about the model hÎ¸(x) =Î¸2
1x2
1 +Î¸2
2x2
2 +
Â· Â· Â· +Î¸2
dx2
d in this subsection, even though itâ€™s not a neural network.
80

81
i-th example ( x(i), y(i)) as
J(i)(Î¸) = 1
2(hÎ¸(x(i)) âˆ’ y(i))2 , (7.1)
and deï¬ne the mean-square cost function for the dataset as
J(Î¸) = 1
n
nâˆ‘
i=1
J(i)(Î¸) , (7.2)
which is same as in linear regression except that we introduce a constant
1/n in front of the cost function to be consistent with the convention. Note
that multiplying the cost function with a scalar will not change the local
minima or global minima of the cost function. Also note that the underlying
parameterization for hÎ¸(x) is diï¬€erent from the case of linear regression,
even though the form of the cost function is the same mean-squared loss.
Throughout the notes, we use the words â€œlossâ€ and â€œcostâ€ interchangeably.
Binary classiï¬cation. Next we deï¬ne the model and loss function for
binary classiï¬cation. Suppose the inputs x âˆˆ Rd. Let Â¯hÎ¸ : Rd â†’ R be a
parameterized model (the analog of Î¸âŠ¤x in logistic linear regression). We
call the output Â¯hÎ¸(x) âˆˆ R the logit. Analogous to Section 2.1, we use the
logistic function g(Â·) to turn the logit Â¯hÎ¸(x) to a probability hÎ¸(x) âˆˆ [0, 1]:
hÎ¸(x) = g(Â¯hÎ¸(x)) = 1/(1 + exp(âˆ’Â¯hÎ¸(x)) . (7.3)
We model the conditional distribution of y given x and Î¸ by
P (y = 1 | x; Î¸) = hÎ¸(x)
P (y = 0 | x; Î¸) = 1 âˆ’ hÎ¸(x)
Following the same derivation in Section 2.1 and using the derivation in
Remark 2.1.1, the negative likelihood loss function is equal to:
J(i)(Î¸) = âˆ’ log p(y(i) | x(i); Î¸) = â„“logistic(Â¯hÎ¸(x(i)), y(i)) (7.4)
As done in equation (7.2), the total loss function is also deï¬ned as the average
of the loss function over individual training examples, J(Î¸) = 1
n
âˆ‘n
i=1 J(i)(Î¸).

82
Multi-class classiï¬cation. Following Section 2.3, we consider a classiï¬ca-
tion problem where the response variable y can take on any one of k values,
i.e. y âˆˆ { 1, 2, . . . , k}. Let Â¯hÎ¸ : Rd â†’ Rk be a parameterized model. We
call the outputs Â¯hÎ¸(x) âˆˆ Rk the logits. Each logit corresponds to the predic-
tion for one of the k classes. Analogous to Section 2.3, we use the softmax
function to turn the logits Â¯hÎ¸(x) into a probability vector with non-negative
entries that sum up to 1:
P (y = j | x; Î¸) =
exp(Â¯hÎ¸(x)j)
âˆ‘k
s=1 exp(Â¯hÎ¸(x)s)
, (7.5)
where Â¯hÎ¸(x)s denotes the s-th coordinate of Â¯hÎ¸(x).
Similarly to Section 2.3, the loss function for a single training example
(x(i), y(i)) is its negative log-likelihood:
J(i)(Î¸) = âˆ’ log p(y(i) | x(i); Î¸) = âˆ’ log
(
exp(Â¯hÎ¸(x(i))y(i))
âˆ‘k
s=1 exp(Â¯hÎ¸(x(i))s)
)
. (7.6)
Using the notations of Section 2.3, we can simply write in an abstract way:
J(i)(Î¸) = â„“ce(Â¯hÎ¸(x(i)), y(i)). (7.7)
The loss function is also deï¬ned as the average of the loss function of indi-
vidual training examples, J(Î¸) = 1
n
âˆ‘n
i=1 J(i)(Î¸).
We also note that the approach above can also be generated to any con-
ditional probabilistic model where we have an exponential distribution for
y, Exponential-family( y; Î·), where Î· = Â¯hÎ¸(x) is a parameterized nonlinear
function of x. However, the most widely used situations are the three cases
discussed above.
Optimizers (SGD). Commonly, people use gradient descent (GD), stochas-
tic gradient (SGD), or their variants to optimize the loss functionJ(Î¸). GDâ€™s
update rule can be written as 2
Î¸ := Î¸ âˆ’ Î±âˆ‡Î¸J(Î¸) (7.8)
where Î± > 0 is often referred to as the learning rate or step size. Next, we
introduce a version of the SGD (Algorithm 1), which is lightly diï¬€erent from
that in the ï¬rst lecture notes.
2Recall that, as deï¬ned in the previous lecture notes, we use the notation â€œ a :=bâ€ to
denote an operation (in a computer program) in which we set the value of a variable a to
be equal to the value of b. In other words, this operation overwrites a with the value of
b. In contrast, we will write â€œ a =bâ€ when we are asserting a statement of fact, that the
value of a is equal to the value of b.

83
Algorithm 1 Stochastic Gradient Descent
1: Hyperparameter: learning rate Î±, number of total iteration niter.
2: Initialize Î¸ randomly.
3: for i = 1 to niter do
4: Sample j uniformly from {1, . . . , n}, and update Î¸ by
Î¸ := Î¸ âˆ’ Î±âˆ‡Î¸J(j)(Î¸) (7.9)
Oftentimes computing the gradient of B examples simultaneously for the
parameter Î¸ can be faster than computing B gradients separately due to
hardware parallelization. Therefore, a mini-batch version of SGD is most
commonly used in deep learning, as shown in Algorithm 2. There are also
other variants of the SGD or mini-batch SGD with slightly diï¬€erent sampling
schemes.
Algorithm 2 Mini-batch Stochastic Gradient Descent
1: Hyperparameters: learning rate Î±, batch size B, # iterations niter.
2: Initialize Î¸ randomly
3: for i = 1 to niter do
4: Sample B examples j1, . . . , jB (without replacement) uniformly from
{1, . . . , n}, and update Î¸ by
Î¸ := Î¸ âˆ’ Î±
B
Bâˆ‘
k=1
âˆ‡Î¸J(jk)(Î¸) (7.10)
With these generic algorithms, a typical deep learning model is learned
with the following steps. 1. Deï¬ne a neural network parametrization hÎ¸(x),
which we will introduce in Section 7.2, and 2. write the backpropagation
algorithm to compute the gradient of the loss function J(j)(Î¸) eï¬ƒciently,
which will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (or
other gradient-based optimizers) with the loss function J(Î¸).

84
7.2 Neural networks
Neural networks refer to a broad type of non-linear models/parametrizations
Â¯hÎ¸(x) that involve combinations of matrix multiplications and other entry-
wise non-linear operations. To have a uniï¬ed treatment for regression prob-
lem and classiï¬cation problem, here we consider Â¯hÎ¸(x) as the output of the
neural network. For regression problem, the ï¬nal prediction hÎ¸(x) = Â¯hÎ¸(x),
and for classiï¬cation problem,Â¯hÎ¸(x) is the logits and the predicted probability
will be hÎ¸(x) = 1/(1+exp( âˆ’Â¯hÎ¸(x)) (see equation 7.3) for binary classiï¬cation
or hÎ¸(x) = softmax(Â¯hÎ¸(x)) for multi-class classiï¬cation (see equation 7.5).
We will start small and slowly build up a neural network, step by step.
A Neural Network with a Single Neuron. Recall the housing price
prediction problem from before: given the size of the house, we want to
predict the price. We will use it as a running example in this subsection.
Previously, we ï¬t a straight line to the graph of size vs. housing price.
Now, instead of ï¬tting a straight line, we wish to prevent negative housing
prices by setting the absolute minimum price as zero. This produces a â€œkinkâ€
in the graph as shown in Figure 7.1. How do we represent such a function
with a single kink as Â¯hÎ¸(x) with unknown parameter? (After doing so, we
can invoke the machinery in Section 7.1.)
We deï¬ne a parameterized function Â¯hÎ¸(x) with input x, parameterized by
Î¸, which outputs the price of the house y. Formally, Â¯hÎ¸ : x â†’ y. Perhaps
one of the simplest parametrization would be
Â¯hÎ¸(x) = max(wx + b, 0), where Î¸ = (w, b) âˆˆ R2 (7.11)
Here Â¯hÎ¸(x) returns a single value: (wx+b) or zero, whichever is greater. In
the context of neural networks, the function max{t, 0} is called a ReLU (pro-
nounced â€œray-luâ€), or rectiï¬ed linear unit, and often denoted by ReLU( t) â‰œ
max{t, 0}.
Generally, a one-dimensional non-linear function that mapsR to R such as
ReLU is often referred to as anactivation function. The model Â¯hÎ¸(x) is said
to have a single neuron partly because it has a single non-linear activation
function. (We will discuss more about why a non-linear activation is called
neuron.)
When the input x âˆˆ Rd has multiple dimensions, a neural network with
a single neuron can be written as
Â¯hÎ¸(x) = ReLU(wâŠ¤x + b), where w âˆˆ Rd, b âˆˆ R, and Î¸ = (w, b) (7.12)

85
500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
100
200
300
400
500
600
700
800
900
1000
housing prices 
square feet 
price (in $1000) 
Figure 7.1: Housing prices with a â€œkinkâ€ in the graph.
The term b is often referred to as the â€œbiasâ€, and the vector w is referred
to as the weight vector. Such a neural network has 1 layer. (We will deï¬ne
what multiple layers mean in the sequel.)
Stacking Neurons. A more complex neural network may take the single
neuron described above and â€œstackâ€ them together such that one neuron
passes its output as input into the next neuron, resulting in a more complex
function.
Let us now deepen the housing prediction example. In addition to the size
of the house, suppose that you know the number of bedrooms, the zip code
and the wealth of the neighborhood. Building neural networks is analogous
to Lego bricks: you take individual bricks and stack them together to build
complex structures. The same applies to neural networks: we take individual
neurons and stack them together to create complex neural networks.
Given these features (size, number of bedrooms, zip code, and wealth),
we might then decide that the price of the house depends on the maximum
family size it can accommodate. Suppose the family size is a function of the
size of the house and number of bedrooms (see Figure 7.2). The zip code
may provide additional information such as how walkable the neighborhood
is (i.e., can you walk to the grocery store or do you need to drive everywhere).
Combining the zip code with the wealth of the neighborhood may predict
the quality of the local elementary school. Given these three derived features
(family size, walkable, school quality), we may conclude that the price of the

86
home ultimately depends on these three features.
Family Size 
School Quality 
Walkable 
Size 
# Bedrooms 
Zip Code 
Wealth 
Price 
y
Figure 7.2: Diagram of a small neural network for predicting housing prices.
Formally, the input to a neural network is a set of input features
x1, x2, x3, x4. We denote the intermediate variables for â€œfamily sizeâ€, â€œwalk-
ableâ€, and â€œschool qualityâ€ by a1, a2, a3 (these aiâ€™s are often referred to as
â€œhidden unitsâ€ or â€œhidden neuronsâ€). We represent each of the aiâ€™s as a neu-
ral network with a single neuron with a subset of x1, . . . , x4 as inputs. Then
as in Figure 7.1, we will have the parameterization:
a1 = ReLU(Î¸1x1 + Î¸2x2 + Î¸3)
a2 = ReLU(Î¸4x3 + Î¸5)
a3 = ReLU(Î¸6x3 + Î¸7x4 + Î¸8)
where (Î¸1, Â· Â· Â· , Î¸8) are parameters. Now we represent the ï¬nal output Â¯hÎ¸(x)
as another linear function with a1, a2, a3 as inputs, and we get 3
Â¯hÎ¸(x) = Î¸9a1 + Î¸10a2 + Î¸11a3 + Î¸12 (7.13)
where Î¸ contains all the parameters ( Î¸1, Â· Â· Â· , Î¸12).
Now we represent the output as a quite complex function of x with pa-
rameters Î¸. Then you can use this parametrization Â¯hÎ¸ with the machinery of
Section 7.1 to learn the parameters Î¸.
Inspiration from Biological Neural Networks. As the name suggests,
artiï¬cial neural networks were inspired by biological neural networks. The
hidden units a1, . . . , am correspond to the neurons in a biological neural net-
work, and the parameters Î¸iâ€™s correspond to the synapses. However, itâ€™s
unclear how similar the modern deep artiï¬cial neural networks are to the bi-
ological ones. For example, perhaps not many neuroscientists think biological
3Typically, for multi-layer neural network, at the end, near the output, we donâ€™t apply
ReLU, especially when the output is not necessarily a positive number.

87
neural networks could have 1000 layers, while some modern artiï¬cial neural
networks do (we will elaborate more on the notion of layers.) Moreover, itâ€™s
an open question whether human brains update their neural networks in a
way similar to the way that computer scientists learn artiï¬cial neural net-
works (using backpropagation, which we will introduce in the next section.).
Two-layer Fully-Connected Neural Networks. We constructed the
neural network in equation (7.13) using a signiï¬cant amount of prior knowl-
edge/belief about how the â€œfamily sizeâ€, â€œwalkableâ€, and â€œschool qualityâ€ are
determined by the inputs. We implicitly assumed that we know the family
size is an important quantity to look at and that it can be determined by
only the â€œsizeâ€ and â€œ# bedroomsâ€. Such a prior knowledge might not be
available for other applications. It would be more ï¬‚exible and general to have
a generic parameterization. A simple way would be to write the intermediate
variable a1 as a function of all x1, . . . , x4:
a1 = ReLU(wâŠ¤
1 x + b1), where w1 âˆˆ R4 and b1 âˆˆ R (7.14)
a2 = ReLU(wâŠ¤
2 x + b2), where w2 âˆˆ R4 and b2 âˆˆ R
a3 = ReLU(wâŠ¤
3 x + b3), where w3 âˆˆ R4 and b3 âˆˆ R
We still deï¬ne Â¯hÎ¸(x) using equation (7.13) with a1, a2, a3 being deï¬ned as
above. Thus we have a so-called fully-connected neural network because
all the intermediate variables aiâ€™s depend on all the inputs xiâ€™s.
For full generality, a two-layer fully-connected neural network with m
hidden units and d dimensional input x âˆˆ Rd is deï¬ned as
âˆ€j âˆˆ [1, ..., m], z j = w[1]
j
âŠ¤
x + b[1]
j where w[1]
j âˆˆ Rd, b[1]
j âˆˆ R (7.15)
aj = ReLU(zj),
a = [a1, . . . , am]âŠ¤ âˆˆ Rm
Â¯hÎ¸(x) = w[2]âŠ¤
a + b[2] where w[2] âˆˆ Rm, b[2] âˆˆ R, (7.16)
Note that by default the vectors in Rd are viewed as column vectors, and
in particular a is a column vector with components a1, a2, ..., am. The indices
[1] and [2] are used to distinguish two sets of parameters: the w[1]
j â€™s (each of
which is a vector in Rd) and w[2] (which is a vector in Rm). We will have
more of these later.
Vectorization. Before we introduce neural networks with more layers and
more complex structures, we will simplify the expressions for neural networks

88
with more matrix and vector notations. Another important motivation of
vectorization is the speed perspective in the implementation. In order to
implement a neural network eï¬ƒciently, one must be careful when using for
loops. The most natural way to implement equation (7.15) in code is perhaps
to use a for loop. In practice, the dimensionalities of the inputs and hidden
units are high. As a result, code will run very slowly if you use for loops.
Leveraging the parallelism in GPUs is/was crucial for the progress of deep
learning.
This gave rise to vectorization. Instead of using for loops, vectorization
takes advantage of matrix algebra and highly optimized numerical linear
algebra packages (e.g., BLAS) to make neural network computations run
quickly. Before the deep learning era, a for loop may have been suï¬ƒcient
on smaller datasets, but modern deep networks and state-of-the-art datasets
will be infeasible to run with for loops.
We vectorize the two-layer fully-connected neural network as below. We
deï¬ne a weight matrix W [1] in RmÃ—d as the concatenation of all the vectors
w[1]
j â€™s in the following way:
W [1] =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
â€” w[1]
1
âŠ¤
â€”
â€” w[1]
2
âŠ¤
â€”
...
â€” w[1]
m
âŠ¤
â€”
ï£¹
ï£ºï£ºï£ºï£ºï£»
âˆˆ RmÃ—d (7.17)
Now by the deï¬nition of matrix vector multiplication, we can write z =
[z1, . . . , zm]âŠ¤ âˆˆ Rm as
ï£®
ï£¯ï£¯ï£¯ï£°
z1
...
...
zm
ï£¹
ï£ºï£ºï£ºï£»
î´™ î´˜î´— î´š
z âˆˆ RmÃ—1
=
ï£®
ï£¯ï£¯ï£¯ï£¯ï£°
â€” w[1]
1
âŠ¤
â€”
â€” w[1]
2
âŠ¤
â€”
...
â€” w[1]
m
âŠ¤
â€”
ï£¹
ï£ºï£ºï£ºï£ºï£»
î´™ î´˜î´— î´š
W [1] âˆˆ RmÃ—d
ï£®
ï£¯ï£¯ï£¯ï£°
x1
x2
...
xd
ï£¹
ï£ºï£ºï£ºï£»
î´™ î´˜î´— î´š
x âˆˆ RdÃ—1
+
ï£®
ï£¯ï£¯ï£¯ï£°
b[1]
1
b[1]
2
...
b[1]
m
ï£¹
ï£ºï£ºï£ºï£»
î´™ î´˜î´— î´š
b[1] âˆˆ RmÃ—1
(7.18)
Or succinctly,
z = W [1]x + b[1] (7.19)
We remark again that a vector in Rd in this notes, following the conventions
previously established, is automatically viewed as a column vector, and can

89
also be viewed as a d Ã— 1 dimensional matrix. (Note that this is diï¬€erent
from numpy where a vector is viewed as a row vector in broadcasting.)
Computing the activations a âˆˆ Rm from z âˆˆ Rm involves an element-
wise non-linear application of the ReLU function, which can be computed in
parallel eï¬ƒciently. Overloading ReLU for element-wise application of ReLU
(meaning, for a vector t âˆˆ Rd, ReLU( t) is a vector such that ReLU( t)i =
ReLU(ti)), we have
a = ReLU(z) (7.20)
Deï¬ne W [2] = [ w[2]âŠ¤
] âˆˆ R1Ã—m similarly. Then, the model in equa-
tion (7.16) can be summarized as
a = ReLU(W [1]x + b[1])
Â¯hÎ¸(x) = W [2]a + b[2] (7.21)
Here Î¸ consists of W [1], W [2] (often referred to as the weight matrices) and
b[1], b[2] (referred to as the biases). The collection of W [1], b[1] is referred to as
the ï¬rst layer, andW [2], b[2] the second layer. The activation a is referred to as
the hidden layer. A two-layer neural network is also called one-hidden-layer
neural network.
Multi-layer fully-connected neural networks. With this succinct no-
tations, we can stack more layers to get a deeper fully-connected neu-
ral network. Let r be the number of layers (weight matrices). Let
W [1], . . . , W[r], b[1], . . . , b[r] be the weight matrices and biases of all the layers.
Then a multi-layer neural network can be written as
a[1] = ReLU(W [1]x + b[1])
a[2] = ReLU(W [2]a[1] + b[2])
Â· Â· Â·
a[râˆ’1] = ReLU(W [râˆ’1]a[râˆ’2] + b[râˆ’1])
Â¯hÎ¸(x) = W [r]a[râˆ’1] + b[r] (7.22)
We note that the weight matrices and biases need to have compatible
dimensions for the equations above to make sense. If a[k] has dimension mk,
then the weight matrix W [k] should be of dimension mk Ã— mkâˆ’1, and the bias
b[k] âˆˆ Rmk. Moreover, W [1] âˆˆ Rm1Ã—d and W [r] âˆˆ R1Ã—mrâˆ’1.

90
The total number of neurons in the network is m1 + Â· Â· Â· + mr, and the
total number of parameters in this network is (d + 1)m1 + (m1 + 1)m2 + Â· Â· Â·+
(mrâˆ’1 + 1)mr.
Sometimes for notational consistency we also write a[0] = x, and a[r] =
hÎ¸(x). Then we have simple recursion that
a[k] = ReLU(W [k]a[kâˆ’1] + b[k]), âˆ€k = 1, . . . , râˆ’ 1 (7.23)
Note that this would have be true for k = r if there were an additional
ReLU in equation (7.22), but often people like to make the last layer linear
(aka without a ReLU) so that negative outputs are possible and itâ€™s easier
to interpret the last layer as a linear model. (More on the interpretability at
the â€œconnection to kernel methodâ€ paragraph of this section.)
Other activation functions. The activation function ReLU can be re-
placed by many other non-linear function Ïƒ(Â·) that maps R to R such as
Ïƒ(z) = 1
1 + eâˆ’z (sigmoid) (7.24)
Ïƒ(z) = ez âˆ’ eâˆ’z
ez + eâˆ’z (tanh) (7.25)
Ïƒ(z) = max{z, Î³z }, Î³ âˆˆ (0, 1) (leaky ReLU) (7.26)
Ïƒ(z) = z
2
[
1 + erf( zâˆš
2)
]
(GELU) (7.27)
Ïƒ(z) = 1
Î² log(1 + exp(Î²z)), Î² > 0 (Softplus) (7.28)
The activation functions are plotted in Figure 7.3. Sigmoid and tanh are
less and less used these days partly because their are bounded from both sides
and the gradient of them vanishes as z goes to both positive and negative
inï¬nity (whereas all the other activation functions still have gradients as the
input goes to positive inï¬nity.) Softplus is not used very often either in
practice and can be viewed as a smoothing of the ReLU so that it has a
proper second order derivative. GELU and leaky ReLU are both variants of
ReLU but they have some non-zero gradient even when the input is negative.
GELU (or its slight variant) is used in NLP models such as BERT and GPT
(which we will discuss in Chapter 14.)
Why do we not use the identity function for Ïƒ(z)? That is, why
not use Ïƒ(z) = z? Assume for sake of argument that b[1] and b[2] are zeros.

91
Figure 7.3: Activation functions in deep learning.
Suppose Ïƒ(z) = z, then for two-layer neural network, we have that
Â¯hÎ¸(x) = W [2]a[1] (7.29)
= W [2]Ïƒ(z[1]) by deï¬nition (7.30)
= W [2]z[1] since Ïƒ(z) = z (7.31)
= W [2]W [1]x from Equation (7.18) (7.32)
= ËœW x where ËœW = W [2]W [1] (7.33)
Notice how W [2]W [1] collapsed into ËœW .
This is because applying a linear function to another linear function will
result in a linear function over the original input (i.e., you can construct a ËœW
such that ËœW x = W [2]W [1]x). This loses much of the representational power
of the neural network as often times the output we are trying to predict
has a non-linear relationship with the inputs. Without non-linear activation
functions, the neural network will simply perform linear regression.
Connection to the Kernel Method. In the previous lectures, we covered
the concept of feature maps. Recall that the main motivation for feature
maps is to represent functions that are non-linear in the input x by Î¸âŠ¤Ï†(x),
where Î¸ are the parameters and Ï†(x), the feature map, is a handcrafted
function non-linear in the raw input x. The performance of the learning
algorithms can signiï¬cantly depends on the choice of the feature map Ï†(x).
Oftentimes people use domain knowledge to design the feature mapÏ†(x) that

92
suits the particular applications. The process of choosing the feature maps
is often referred to as feature engineering.
We can view deep learning as a way to automatically learn the right
feature map (sometimes also referred to as â€œthe representationâ€) as follows.
Suppose we denote by Î² the collection of the parameters in a fully-connected
neural networks (equation (7.22)) except those in the last layer. Then we
can abstract right a[râˆ’1] as a function of the input x and the parameters in
Î²: a[râˆ’1] = Ï†Î²(x). Now we can write the model as
Â¯hÎ¸(x) = W [r]Ï†Î²(x) + b[r] (7.34)
When Î² is ï¬xed, then Ï†Î²(Â·) can viewed as a feature map, and therefore Â¯hÎ¸(x)
is just a linear model over the features Ï†Î²(x). However, we will train the
neural networks, both the parameters in Î² and the parameters W [r], b[r] are
optimized, and therefore we are not learning a linear model in the feature
space, but also learning a good feature map Ï†Î²(Â·) itself so that itâ€™s possi-
ble to predict accurately with a linear model on top of the feature map.
Therefore, deep learning tends to depend less on the domain knowledge of
the particular applications and requires often less feature engineering. The
penultimate layer a[r] is often (informally) referred to as the learned features
or representations in the context of deep learning.
In the example of house price prediction, a fully-connected neural network
does not need us to specify the intermediate quantity such â€œfamily sizeâ€, and
may automatically discover some useful features in the last penultimate layer
(the activation a[râˆ’1]), and use them to linearly predict the housing price.
Often the feature map / representation obtained from one datasets (that is,
the function Ï†Î²(Â·) can be also useful for other datasets, which indicates they
contain essential information about the data. However, oftentimes, the neural
network will discover complex features which are very useful for predicting
the output but may be diï¬ƒcult for a human to understand or interpret. This
is why some people refer to neural networks as a black box , as it can be
diï¬ƒcult to understand the features it has discovered.
7.3 Modules in Modern Neural Networks
The multi-layer neural network introduced in equation (7.22) of Section 7.2
is often called multi-layer perceptron (MLP) these days. Modern neural net-
works used in practice are often much more complex and consist of multiple
building blocks or multiple layers of building blocks. In this section, we will

93
introduce some of the other building blocks and discuss possible ways to
combine them.
First, each matrix multiplication can be viewed as a building block. Con-
sider a matrix multiplication operation with parameters ( W, b) where W is
the weight matrix and b is the bias vector, operating on an input z,
MMW,b(z) = W z + b . (7.35)
Note that we implicitly assume all the dimensions are chosen to be compat-
ible. We will also drop the subscripts under MM when they are clear in the
context or just for convenience when they are not essential to the discussion.
Then, the MLP can be written as as a composition of multiple matrix
multiplication modules and nonlinear activation modules (which can also be
viewed as a building block):
MLP(x) = MMW [r],b[r](Ïƒ(MMW [râˆ’1],b[râˆ’1](Ïƒ(Â· Â· Â·MMW [1],b[1](x)))). (7.36)
Alternatively, when we drop the subscripts that indicate the parameters for
convenience, we can write
MLP(x) = MM(Ïƒ(MMÏƒ(Â· Â· Â·MM(x)))). (7.37)
Note that in this lecture notes, by default, all the modules have diï¬€erent
sets of parameters, and the dimensions of the parameters are chosen such
that the composition is meaningful.
Larger modules can be deï¬ned via smaller modules as well, e.g., one
activation layer Ïƒ and a matrix multiplication layer MM are often combined
and called a â€œlayerâ€ in many papers. People often draw the architecture
with the basic modules in a ï¬gure by indicating the dependency between
these modules. E.g., see an illustration of an MLP in Figure 7.4, Left.
Residual connections. One of the very inï¬‚uential neural network archi-
tecture for vision application is ResNet, which uses the residual connections
that are essentially used in almost all large-scale deep learning architectures
these days. Using our notation above, a very much simpliï¬ed residual block
can be deï¬ned as
Res(z) = z + Ïƒ(MM(Ïƒ(MM(z)))). (7.38)
A much simpliï¬ed ResNet is a composition of many residual blocks followed
by a matrix multiplication,
ResNet-S(x) = MM(Res(Res(Â· Â· Â·Res(x)))). (7.39)

94
ğ‘¥
Layer ğ‘Ÿâˆ’1
Layer ğ‘–...Layer 1
MLP(ğ‘¥)...Layerğ‘–
MM!["],#["]
ğœ
MM![$],#[$]
ğ‘¥
Res
Res...Res
ResNet-S(ğ‘¥)...Res
MMğœMMğœ
Figure 7.4: Illustrative Figures for Architecture. Left: An MLP with r
layers. Right: A residual network.
We also draw the dependency of these modules in Figure 7.4, Right.
We note that the ResNet-S is still not the same as the ResNet architec-
ture introduced in the seminal paper [He et al., 2016] because ResNet uses
convolution layers instead of vanilla matrix multiplication, and adds batch
normalization between convolutions and activations. We will introduce con-
volutional layers and some variants of batch normalization below. ResNet-S
and layer normalization are part of the Transformer architecture that are
widely used in modern large language models.
Layer normalization. Layer normalization, denoted by LN in this text,
is a module that maps a vector z âˆˆ Rm to a more normalized vector LN(z) âˆˆ
Rm. It is oftentimes used after the nonlinear activations.
We ï¬rst deï¬ne a sub-module of the layer normalization, denoted by LN-S.
LN-S(z) =
ï£®
ï£¯ï£¯ï£¯ï£°
z1âˆ’Ë†Âµ
Ë†Ïƒz2âˆ’Ë†Âµ
Ë†Ïƒ
...
zmâˆ’Ë†Âµ
Ë†Ïƒ
ï£¹
ï£ºï£ºï£ºï£» , (7.40)
where Ë†Âµ =
âˆ‘m
i=1 zi
m is the empirical mean of the vector z and Ë†Ïƒ =
âˆš âˆ‘m
i=1(ziâˆ’Ë†Âµ2)
m
is the empirical standard deviation of the entries of z.4 Intuitively, LN-S(z)
is a vector that is normalized to having empirical mean zero and empirical
standard deviation 1.
4Note that we divide by m instead of m âˆ’ 1 in the empirical standard deviation here
because we are interested in making the output of LN-S( z) have sum of squares equal to
1 (as opposed to estimating the standard deviation in statistics.)

95
Oftentimes zero mean and standard deviation 1 is not the most desired
normalization scheme, and thus layernorm introduces to parameters learnable
scalars Î² and Î³ as the desired mean and standard deviation, and use an aï¬ƒne
transformation to turn the output of LN-S( z) into a vector with mean Î² and
standard deviation Î³.
LN(z) = Î² + Î³ Â· LN-S(z) =
ï£®
ï£¯ï£¯ï£¯ï£°
Î² + Î³
(z1âˆ’Ë†Âµ
Ë†Ïƒ
)
Î² + Î³
(z2âˆ’Ë†Âµ
Ë†Ïƒ
)
...
Î² + Î³
(zmâˆ’Ë†Âµ
Ë†Ïƒ
)
ï£¹
ï£ºï£ºï£ºï£» . (7.41)
Here the ï¬rst occurrence of Î² should be technically interpreted as a vector
with all the entries being Î². in We also note that Ë†Âµ and Ë†Ïƒ are also functions
of z and shouldnâ€™t be treated as constants when computing the derivatives of
layernorm. Moreover, Î² and Î³ are learnable parameters and thus layernorm
is a parameterized module (as opposed to the activation layer which doesnâ€™t
have any parameters.)
Scaling-invariant property. One important property of layer normalization
is that it will make the model invariant to scaling of the parameters in the
following sense. Suppose we consider composing LN with MM W,b and get
a subnetwork LN(MM W,b(z)). Then, we have that the output of this sub-
network does not change when the parameter in MM W,b is scaled:
LN(MMÎ±W,Î±b(z)) = LN(MMW,b(z)), âˆ€Î± > 0. (7.42)
To see this, we ï¬rst know that LN-S( Â·) is scale-invariant
LN-S(Î±z) =
ï£®
ï£¯ï£¯ï£¯ï£°
Î±z1âˆ’Î±Ë†Âµ
Î±Ë†ÏƒÎ±z2âˆ’Î±Ë†Âµ
Î±Ë†Ïƒ
...
Î±zmâˆ’Î±Ë†Âµ
Î±Ë†Ïƒ
ï£¹
ï£ºï£ºï£ºï£» =
ï£®
ï£¯ï£¯ï£¯ï£°
z1âˆ’Ë†Âµ
Ë†Ïƒz2âˆ’Ë†Âµ
Ë†Ïƒ
...
zmâˆ’Ë†Âµ
Ë†Ïƒ
ï£¹
ï£ºï£ºï£ºï£» = LN-S(z). (7.43)
Then we have
LN(MMÎ±W,Î±b(z)) = Î² + Î³LN-S(MMÎ±W,Î±b(z)) (7.44)
= Î² + Î³LN-S(Î±MMW,b(z)) (7.45)
= Î² + Î³LN-S(MMW,b(z)) (7.46)
= LN(MMW,b(z)). (7.47)
Due to this property, most of the modern DL architectures for large-scale
computer vision and language applications have the following scale-invariant

96
property w.r.t all the weights that are not at the last layer. Suppose the
network f has last layerâ€™ weights Wlast, and all the rest of the weights are
denote by W . Then, we have fWlast,Î±W (x) = fWlast,W (x) for all Î± > 0. Here,
the last layers weights are special because there are typically no layernorm
or batchnorm after the last layerâ€™s weights.
Other normalization layers. There are several other normalization layers that
aim to normalize the intermediate layers of the neural networks to a more
ï¬xed and controllable scaling, such as batch-normalization [ ?], and group
normalization [?]. Batch normalization and group normalization are more
often used in computer vision applications whereas layer norm is used more
often in language applications.
Convolutional Layers. Convolutional Neural Networks are neural net-
works that consist of convolution layers (and many other modules), and are
particularly useful for computer vision applications. For the simplicity of
exposition, we focus on 1-D convolution in this text and only brieï¬‚y mention
2-D convolution informally at the end of this subsection. (2-D convolution
is more suitable for images which have two dimensions. 1-D convolution is
also used in natural language processing.)
We start by introducing a simpliï¬ed version of the 1-D convolution layer,
denoted by Conv1D-S(Â·) which is a type of matrix multiplication layer with
a special structure. The parameters of Conv1D-S are a ï¬lter vector w âˆˆ Rk
where k is called the ï¬lter size (oftentimes k â‰ª m), and a bias scalar b.
Oftentimes the ï¬lter is also called a kernel (but it does not have much to do
with the kernel in kernel method.) For simplicity, we assume k = 2â„“ + 1 is
an odd number. We ï¬rst pad zeros to the input vector z in the sense that we
let z1âˆ’â„“ = z1âˆ’â„“+1 = .. = z0 = 0 and zm+1 = zm+2 = .. = zm+â„“ = 0, and treat
z as an (m + 2â„“)-dimension vector. Conv1D-S outputs a vector of dimension
Rm where each output dimension is a linear combination of subsets of zjâ€™s
with coeï¬ƒcients from w,
Conv1D-S(z)i = w1ziâˆ’â„“ + w2ziâˆ’â„“+1 + Â· Â· Â· + w2â„“+1zi+â„“ =
2â„“+1âˆ‘
j=1
wjziâˆ’â„“+(jâˆ’1).
(7.48)
Therefore, one can view Conv1D-S as a matrix multiplication with shared

97
parameters: Conv1D-S( z) = Qz, where
Q =
ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°
wâ„“+1 Â·Â·Â· w2â„“+1 0 0 Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· 0
wâ„“ Â·Â·Â· w2â„“ w2â„“+1 0 Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· 0
...
w1 Â·Â·Â· wâ„“+1 Â·Â·Â· Â·Â·Â· Â·Â·Â· w2â„“+1 0 Â·Â·Â· Â·Â·Â· Â·Â·Â· 0
0 w1 Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· w2â„“ w2â„“+1 0 Â·Â·Â· Â·Â·Â· 0
...
...
0 Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· 0 w1 Â·Â·Â· Â·Â·Â· w2â„“+1
...
0 Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· 0 w1 Â·Â·Â· wâ„“+1
ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
. (7.49)
Note that Qi,j = Qiâˆ’1,jâˆ’1 for all i, j âˆˆ {2, . . . , m}, and thus convoluation is a
matrix multiplication with parameter sharing. We also note that computing
the convolution only takes O(km) times but computing a generic matrix
multiplication takes O(m2) time. Convolution has k parameters but generic
matrix multiplication will havem2 parameters. Thus convolution is supposed
to be much more eï¬ƒcient than a generic matrix multiplication (as long as
the additional structure imposed does not hurt the ï¬‚exibility of the model
to ï¬t the data).
We also note that in practice there are many variants of the convolutional
layers that we deï¬ne here, e.g., there are other ways to pad zeros or sometimes
the dimension of the output of the convolutional layers could be diï¬€erent from
the input. We omit some of this subtleties here for simplicity.
The convolutional layers used in practice have also many â€œchannelsâ€ and
the simpliï¬ed version above corresponds to the 1-channel version. Formally,
Conv1D takes in C vectors z1, . . . , zC âˆˆ Rm as inputs, where C is referred
to as the number of channels. In other words, the more general version,
denoted by Conv1D, takes in a matrix as input, which is the concatenation
of z1, . . . , zC and has dimension mÃ—C. It can output Câ€² vectors of dimension
m, denoted by Conv1D( z)1, . . . ,Conv1D(z)Câ€², where Câ€² is referred to as the
output channel, or equivalently a matrix of dimension m Ã— Câ€². Each of the
output is a sum of the simpliï¬ed convolutions applied on various channels.
âˆ€i âˆˆ [Câ€²], Conv1D(z)i =
Câˆ‘
j=1
Conv1D-Si,j(zj). (7.50)
Note that each Conv1D-S i,j are modules with diï¬€erent parameters, and
thus the total number of parameters is k (the number of parameters in a
Conv1D-S) Ã—CCâ€² (the number of Conv1D-S i.jâ€™s) = kCCâ€². In contrast, a
generic linear mapping from RmÃ—C and RmÃ—Câ€²
has m2CCâ€² parameters. The

98
parameters can also be represented as a three-dimensional tensor of dimen-
sion k Ã— C Ã— Câ€².
2-D convolution (brief). A 2-D convolution with one channel, denoted by
Conv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional input
z âˆˆ RmÃ—m and applies a ï¬lter of size k Ã— k, and outputs Conv2D-S( z) âˆˆ
RmÃ—m. The full 2-D convolutional layer, denoted by Conv2D, takes in
a sequence of matrices z1, . . . , zC âˆˆ RmÃ—m, or equivalently a 3-D ten-
sor z = ( z1, . . . , zC) âˆˆ RmÃ—mÃ—C and outputs a sequence of matrices,
Conv2D(z)1, . . . ,Conv2D(z)Câ€² âˆˆ RmÃ—m, which can also be viewed as a 3D
tensor in RmÃ—mÃ—Câ€²
. Each channel of the output is sum of the outcomes of
applying Conv2D-S layers on all the input channels.
âˆ€i âˆˆ [Câ€²], Conv2D(z)i =
Câˆ‘
j=1
Conv2D-Si,j(zj). (7.51)
Because there are CCâ€² number of Conv2D-S modules and each of the
Conv2D-S module has k2 parameters, the total number of parameters is
CCâ€²k2. The parameters can also be viewed as a 4D tensor of dimension
C Ã— Câ€² Ã— k Ã— k.
7.4 Backpropagation
In this section, we introduce backpropgation or auto-diï¬€erentiation, which
computes the gradient of the loss âˆ‡J(Î¸) eï¬ƒciently. We will start with an
informal theorem that states that as long as a real-valued function f can be
eï¬ƒciently computed/evaluated by a diï¬€erentiable network or circuit, then its
gradient can be eï¬ƒciently computed in a similar time. We will then show
how to do this concretely for neural networks.
Because the formality of the general theorem is not the main focus here,
we will introduce the terms with informal deï¬nitions. By a diï¬€erentiable
circuit or a diï¬€erentiable network, we mean a composition of a sequence of
diï¬€erentiable arithmetic operations (additions, subtraction, multiplication,
divisions, etc) and elementary diï¬€erentiable functions (ReLU, exp, log, sin,
cos, etc.). Let the size of the circuit be the total number of such operations
and elementary functions. We assume that each of the operations and func-
tions, and their derivatives or partial derivatives ecan be computed in O(1)
time.
Theorem 7.4.1: [backpropagation or auto-diï¬€erentiation, informally stated]
Suppose a diï¬€erentiable circuit of size N computes a real-valued function

99
f : Râ„“ â†’ R. Then, the gradient âˆ‡f can be computed in time O(N), by a
circuit of size O(N).5
We note that the loss function J(j)(Î¸) for j-th example can be indeed
computed by a sequence of operations and functions involving additions,
subtraction, multiplications, and non-linear activations. Thus the theorem
suggests that we should be able to compute the âˆ‡J(j)(Î¸) in a similar time
to that for computing J(j)(Î¸) itself. This does not only apply to the fully-
connected neural network introduced in the Section 7.2, but also many other
types of neural networks that uses more advance modules.
We remark that auto-diï¬€erentiation or backpropagation is already imple-
mented in all the deep learning packages such as tensorï¬‚ow and pytorch, and
thus in practice, in most of cases a researcher does not need to write their
backpropagation algorithms. However, understanding it is very helpful for
gaining insights into the working of deep learning.
Organization of the rest of the section. In Section 7.4.1, we will start review-
ing the basic Chain rule with a new perspective that is particularly useful
for understanding backpropgation. Section 7.4.2 will introduce the general
strategy for backpropagation. Section 7.4.2 will discuss how to compute the
so-called backward function for basic modules used in neural networks, and
Section 7.4.4 will put everything together to get a concrete backprop algo-
rithm for MLPs.
7.4.1 Preliminaries on partial derivatives
Suppose a scalar variable J depend on some variables z (which could be a
scalar, matrix, or high-order tensor), we write âˆ‚J
âˆ‚z as the partial derivatives
of J w.r.t to the variable z. We stress that the convention here is that âˆ‚J
âˆ‚z
has exactly the same dimension as z itself. For example, if z âˆˆ RmÃ—n, then
âˆ‚J
âˆ‚z âˆˆ RmÃ—n, and the ( i, j)-entry of âˆ‚J
âˆ‚z is equal to âˆ‚J
âˆ‚zij
.
Remark 7.4.2 :When both J and z are not scalars, the partial derivatives of
J w.r.t z becomes either a matrix or tensor and the notation becomes some-
what tricky. Besides the mathematical or notational challenges in dealing
5We note if the output of the function f does not depend on some of the input co-
ordinates, then we set by default the gradient w.r.t that coordinate to zero. Setting to
zero does not count towards the total runtime here in our accounting scheme. This is why
whenN â‰¤â„“, we can compute the gradient in O(N) time, which might be potentially even
less than â„“.

100
with these partial derivatives of multi-variate functions, they are also expen-
sive to compute and store, and thus rarely explicitly constructed empirically.
The experience of authors of this note is that itâ€™s generally more productive
to think only about derivatives of scalar function w.r.t to vector, matrices,
or tensors. For example, in this note, we will not deal with derivatives of
multi-variate functions.
Chain rule. We review the chain rule in calculus but with a perspective
and notions that are more relevant for auto-diï¬€erentiation.
Consider a scalar variable J which is obtained by the composition of f
and g on some variable z,
z âˆˆ Rm
u = g(z) âˆˆ Rn
J = f(u) âˆˆ R . (7.52)
The same derivations below can be easily extend to the cases when z and u
are matrices or tensors; but we insist that the ï¬nal variableJ is a scalar. (See
also Remark 7.4.2.) Let u = (u1, . . . , un) and let g(z) = ( g1(z), Â· Â· Â· , gn(z)).
Then, the standard chain rule gives us that
âˆ€i âˆˆ {1, . . . , m}, âˆ‚J
âˆ‚zi
=
nâˆ‘
j=1
âˆ‚J
âˆ‚uj
Â· âˆ‚gj
âˆ‚zi
. (7.53)
Alternatively, when z and u are both vectors, in a vectorized notation:
âˆ‚J
âˆ‚z =
ï£®
ï£¯ï£°
âˆ‚g1
âˆ‚z1
Â· Â· Â· âˆ‚gn
âˆ‚z1
... ... ...
âˆ‚g1
âˆ‚zm
Â· Â· Â· âˆ‚gn
âˆ‚zm
ï£¹
ï£ºï£» Â· âˆ‚J
âˆ‚u . (7.54)
In other words, the backward function is always a linear map from âˆ‚J
âˆ‚u to
âˆ‚J
âˆ‚z , though note that the mapping itself can depend on z in complex ways.
The matrix on the RHS of (7.54) is actually the transpose of the Jacobian
matrix of the function g. However, we do not discuss in-depth about Jacobian
matrices to avoid complications. Part of the reason is that when z is a matrix
(or tensor), to write an analog of equation (7.54), one has to either ï¬‚atten z
into a vector or introduce additional notations on tensor-matrix product. In
this sense, equation (7.53) is more convenient and eï¬€ective to use in all cases.
For example, when z âˆˆ RrÃ—s is a matrix, we can easily rewrite equation (7.53)

101
to
âˆ€i, k, âˆ‚J
âˆ‚zik
=
nâˆ‘
j=1
âˆ‚J
âˆ‚uj
Â· âˆ‚gj
âˆ‚zik
. (7.55)
which will indeed be used in some of the derivations in Section 7.4.3.
Key interpretation of the chain rule. We can view the formula above (equa-
tion (7.53) or (7.54)) as a way to compute âˆ‚J
âˆ‚z from âˆ‚J
âˆ‚u . Consider the following
abstract problem. Suppose J depends on z via u as deï¬ned in equation (7.52).
However, suppose the function f is not given or the function f is complex,
but we are given the value of âˆ‚J
âˆ‚u . Then, the formula in equation (7.54) gives
us a way to compute âˆ‚J
âˆ‚z from âˆ‚J
âˆ‚u .
âˆ‚J
âˆ‚u
chain rule, formula (7.54)
= = = = = = = = = = = = = = = = = = = =â‡’
only requires info about g(Â·) and z
âˆ‚J
âˆ‚z . (7.56)
Moreover, this formula only involves knowledge about g (more precisely âˆ‚gj
âˆ‚zi
).
We will repeatedly use this fact in situations where g is a building blocks of
a complex network f.
Empirically, itâ€™s often useful to modularized the mapping in (7.53) or
(7.54) into a black-box, and mathematically itâ€™s also convenient to deï¬ne a
notation for it. 6 We use B[g, z] to deï¬ne the function that maps âˆ‚J
âˆ‚u to âˆ‚J
âˆ‚z ,
and write
âˆ‚J
âˆ‚z = B[g, z]
(âˆ‚J
âˆ‚u
)
. (7.57)
We call B[g, z] the backward function for the module g. Note that when z
is ï¬xed, B[g, z] is merely a linear map from Rn to Rm. Using equation (7.53),
we have
(B[g, z](v))i =
mâˆ‘
j=1
âˆ‚gj
âˆ‚zi
Â· vj . (7.58)
Or in vectorized notation, using (7.54), we have
B[g, z](v) =
ï£®
ï£¯ï£°
âˆ‚g1
âˆ‚z1
Â· Â· Â· âˆ‚gn
âˆ‚z1
... ... ...
âˆ‚g1
âˆ‚zm
Â· Â· Â· âˆ‚gn
âˆ‚zm
ï£¹
ï£ºï£» Â· v . (7.59)
6e.g., the function is the .backward() method of the module in pytorch.

102
and therefore B[g, z] can be viewed as a matrix. However, in reality, z will be
changing and thus the backward mapping has to be recomputed for diï¬€erent
zâ€™s while g is often ï¬xed. Thus, empirically, the backward function B[g, z](v)
is often viewed as a function which takes in z (=the input to g) and v (=a
vector that is supposed to be the gradient of some variable J w.r.t to the
output of g) as the inputs, and outputs a vector that is supposed to be the
gradient of J w.r.t to z.
7.4.2 General strategy of backpropagation
We discuss the general strategy of auto-diï¬€erentiation in this section to build
a high-level understanding. Then, we will instantiate the approach to con-
crete neural networks. We take the viewpoint that neural networks are com-
plex compositions of small building blocks such as MM, Ïƒ, Conv2D, LN,
etc., deï¬ned in Section 7.3. Note that the losses (e.g., mean-squared loss, or
the cross-entropy loss) can also be abstractly viewed as additional modules.
Thus, we can abstractly write the loss function J (on a single example (x, y))
as a composition of many modules: 7
J = Mk(Mkâˆ’1(Â· Â· Â· M1(x))) . (7.60)
For example, for a binary classiï¬cation problem with a MLP Â¯hÎ¸(x) (de-
ï¬ned in equation (7.36) and (7.37)), the loss function has ber written in the
form of equation (7.60) with M1 = MM W [1],b[1], M2 = Ïƒ, M3 = MM W [2],b[2],
. . ., and Mkâˆ’1 = MMW [r],b[r] and Mk = â„“logistic.
We can see from this example that some modules involve parameters, and
other modules might only involve a ï¬xed set of operations. For generality,
we assume that eachj Mi involves a set of parameters Î¸[i], though Î¸[i] could
possibly be an empty set when Mi is a ï¬xed operation such as the nonlinear
activations. We will discuss more on the granularity of the modularization,
but so far we assume all the modules Miâ€™s are simple enough.
We introduce the intermediate variables for the computation in (7.60).
7Technically, we should writeJ =Mk(Mkâˆ’1(Â· Â· Â·M1(x)),y ). However, y is treated as a
constant for the purpose of computing the derivatives w.r.t to the parameters, and thus
we can view it as part of Mk for the sake of simplicity of notations.

103
Let
u[0] = x
u[1] = M1(u[0])
u[2] = M2(u[1])
...
J = u[k] = Mk(u[kâˆ’1]) . (F)
Backpropgation consists of two passes, the forward pass and backward
pass. In the forward pass, the algorithm simply computes u[1], . . . , u[k] from
i = 1, . . . , k, sequentially using the deï¬nition in (F), and save all the in-
termediate variables u[i]â€™s in the memory.
In the backward pass , we ï¬rst compute the derivatives w.r.t to the
intermediate variables, that is, âˆ‚J
âˆ‚u[k] , . . . , âˆ‚J
âˆ‚u[1] , sequentially in this backward
order, and then compute the derivatives of the parameters âˆ‚J
âˆ‚Î¸ [i] from âˆ‚J
âˆ‚u[i] and
u[iâˆ’1]. These two type of computations can be also interleaved with each
other because âˆ‚J
âˆ‚Î¸ [i] only depends on âˆ‚J
âˆ‚u[i] and u[iâˆ’1] but not any âˆ‚J
âˆ‚u[k] with
k < i .
We ï¬rst see why âˆ‚J
âˆ‚u[iâˆ’1] can be computed eï¬ƒciently from âˆ‚J
âˆ‚u[i] and u[iâˆ’1]
by invoking the discussion in Section 7.4.1 on the chain rule. We in-
stantiate the discussion by setting u = u[i] and z = u[iâˆ’1], and f(u) =
Mk(Mkâˆ’1(Â· Â· Â· Mi+1(u[i]))), and g(Â·) = Mi(Â·). Note that f is very complex
but we donâ€™t need any concrete information about f. Then, the conclusive
equation (7.56) corresponds to
âˆ‚J
âˆ‚u[i]
chain rule
= = = = = = = = = = = = = = = = = = = = = = = = = =â‡’
only requires info about Mi(Â·) and u[iâˆ’1]
âˆ‚J
âˆ‚u[iâˆ’1] . (7.61)
More precisely, we can write, following equation (7.57)
âˆ‚J
âˆ‚u[iâˆ’1] = B[Mi, u[iâˆ’1]]
( âˆ‚J
âˆ‚u[i]
)
. (B1)
Instantiating the chain rule with z = Î¸[i] and u = u[i], we also have
âˆ‚J
âˆ‚Î¸[i] = B[Mi, Î¸[i]]
( âˆ‚J
âˆ‚u[i]
)
. (B2)
See Figure 7.5 for an illustration of the algorithm.

104
ğ‘¥ ...
ğ‘€!
ğ½...ğ‘€"
ğ‘¢[!]
ğ‘¢["%!]
ğœ•ğ½ğœ•ğ½â„¬[ğ‘€",ğ‘¢["%!]]ğœ•ğ½ğœ•ğ‘¢!"#
ğ‘¢[&]
ğ‘€&
ğ‘¢[&%!]
â„¬[ğ‘€&,ğ‘¢[&%!]]ğœ•ğ½ğœ•ğ‘¢$"#
ğœ•ğ½ğœ•ğ‘¢$ ...
ğœ•ğ½ğœ•ğ‘¢# ...
Forward passBackward pass
â„¬[ğ‘€!,ğœƒ!]ğœ•ğ½ğœ•ğœƒ#
â„¬[ğ‘€&,ğœƒ&]ğœ•ğ½ğœ•ğœƒ$
â„¬[ğ‘€",ğœƒ"]ğœ•ğ½ğœ•ğœƒ!
Figure 7.5: Back-propagation.
Remark 7.4.3: [Computational eï¬ƒciency and granularity of the modules]
The main underlying purpose of treating a complex network as compositions
of small modules is that small modules tend to have eï¬ƒciently implementable
backward function. In fact, the backward functions of all the atomic modules
such as addition, multiplication and ReLU can be computed as eï¬ƒciently as
the the evaluation of these modules (up to multiplicative constant factor).
Using this fact, we can prove Theorem 7.4.1 by viewing neural networks as
compositions of many atomic operations, and invoking the backpropagation
discussed above. However, in practice, itâ€™s oftentimes more convenient to
modularize the networks using modules on the level of matrix multiplication,
layernorm, etc. As we will see, naive implementation of these operationsâ€™
backward functions also have the same runtime as the evaluation of these
functions.

105
7.4.3 Backward functions for basic modules
Using the general strategy in Section 7.4.2, it suï¬ƒces to compute the back-
ward function for all modules Miâ€™s used in the networks. We compute the
backward function for the basic module MM, activationsÏƒ, and loss functions
in this section.
Backward function for MM. Suppose MMW,b(z) = W z + b is a matrix multi-
plication module where z âˆˆ Rm and W âˆˆ RnÃ—m. Then, using equation (7.59),
we have for v âˆˆ Rn
B[MM, z](v) =
ï£®
ï£¯ï£°
âˆ‚(W z+b)1
âˆ‚z1
Â· Â· Â· âˆ‚(W z+b)n
âˆ‚z1
... ... ...
âˆ‚(W z+b)1
âˆ‚zm
Â· Â· Â· âˆ‚(W z+b)n
âˆ‚zm
ï£¹
ï£ºï£» v . (7.62)
Using the fact that âˆ€i âˆˆ [m], j âˆˆ [n], âˆ‚(W z+b)j
âˆ‚zi
= âˆ‚bj+âˆ‘m
k=1 Wjk zk
âˆ‚zi
= Wji, we
have
B[MM, z](v) = WâŠ¤v âˆˆ Rm . (7.63)
In the derivation above, we have treated MM as a function of z. If we treat
MM as a function of W and b, then we can also compute the backward
function for the parameter variables W and b. Itâ€™s less convenient to use
equation (7.59) because the variable W is a matrix and the matrix in (7.59)
will be a 4-th order tensor that is challenging for us to mathematically write
down. We use (7.58) instead:
(B[MM, W](v))ij =
mâˆ‘
k=1
âˆ‚(W z + b)k
âˆ‚Wij
Â· vk =
mâˆ‘
k=1
âˆ‚ âˆ‘m
s=1 Wkszs
âˆ‚Wij
Â· vk = vizj .
(7.64)
In vectorized notation, we have
B[MM, W](v) = vzâŠ¤ âˆˆ RnÃ—Ã—m . (7.65)
Using equation (7.59) for the variable b, we have,
B[MM, b](v) =
ï£®
ï£¯ï£°
âˆ‚(W z+b)1
âˆ‚b1
Â· Â· Â· âˆ‚(W z+b)n
âˆ‚b1
... ... ...
âˆ‚(W z+b)1
âˆ‚bn
Â· Â· Â· âˆ‚(W z+b)n
âˆ‚bn
ï£¹
ï£ºï£» v = v . (7.66)

106
Here we used that âˆ‚(W z+b)j
âˆ‚bi
= 0 if i Ì¸= j and âˆ‚(W z+b)j
âˆ‚bi
= 1 if i = j.
The computational eï¬ƒciency for computing the backward function is
O(mn), the same as evaluating the result of matrix multiplication up to
constant factor.
Backward function for the activations. Suppose M(z) = Ïƒ(z) where Ïƒ is an
element-wise activation function and z âˆˆ Rm. Then, using equation (7.59),
we have
B[Ïƒ, z](v) =
ï£®
ï£¯ï£°
âˆ‚Ïƒ(z1)
âˆ‚z1
Â· Â· Â· âˆ‚Ïƒ(zm)
âˆ‚z1
... ... ...
âˆ‚Ïƒ(z1)
âˆ‚zm
Â· Â· Â· âˆ‚Ïƒ(zm)
âˆ‚zm
ï£¹
ï£ºï£» v (7.67)
= diag(Ïƒâ€²(z1), Â· Â· Â· , Ïƒâ€²(zm))v (7.68)
= Ïƒâ€²(z) âŠ™ v âˆˆ Rm . (7.69)
Here, we used the fact that âˆ‚Ïƒ(zj)
âˆ‚zi
= 0 when j Ì¸= i, diag(Î»1, . . . , Î»m) denotes
the diagonal matrix with Î»1, . . . , Î»m on the diagonal, and âŠ™ denotes the
element-wise product of two vectors with the same dimension, and Ïƒâ€²(Â·) is
the element-wise application of the derivative of the activation function Ïƒ.
Regarding computation eï¬ƒciency, we note that at the ï¬rst sight, equa-
tion (7.67) appears to indicate the backward function takes O(m2) time, but
equation (7.69) shows that itâ€™s implementable in O(m) time (which is the
same as the time for evaluating of the function.) We are not supposed to be
surprised by that the possibility of simplifying equation (7.67) to (7.69)â€”if
we use smaller modules, that is, treating the vector-to-vector nonlinear ac-
tivation as m scalar-to-scalar non-linear activation, then itâ€™s more obvious
that the backward pass should have similar time to the forward pass.
Backward function for loss functions. When a module M takes in a vector
z and outputs a scalar, by equation (7.59), the backward function takes in a
scalar v and outputs a vector with entries ( B[M, z](v))i = âˆ‚M
âˆ‚zi
v. Therefore,
in vectorized notation, B[M, z](v) = âˆ‚M
âˆ‚z Â· v.
Recall that squared loss â„“MSE(z, y) = 1
2(z âˆ’ y)2. Thus, B[â„“MSE, z](v) =
âˆ‚ 1
2 (zâˆ’y)2
âˆ‚z Â· v = (z âˆ’ y) Â· v.
For logistics loss, by equation (2.6), we have
B[â„“logistic, t](v) = âˆ‚â„“logistic(t, y)
âˆ‚t Â· v = (1/(1 + exp(âˆ’t)) âˆ’ y) Â· v . (7.70)

107
For cross-entropy loss, by equation (2.17), we have
B[â„“ce, t](v) = âˆ‚â„“ce(t, y)
âˆ‚t Â· v = (Ï† âˆ’ ey) Â· v , (7.71)
where Ï† = softmax(t).
7.4.4 Back-propagation for MLPs
Given the backward functions for every module needed in evaluating the loss
of an MLP, we follow the strategy in Section 7.4.2 to compute the gradient
of the loss w.r.t to the hidden activations and the parameters.
We consider the an r-layer MLP with a logistic loss. The loss function
can be computed via a sequence of operations (that is, the forward pass),
z[1] = MMW [1],b[1](x),
a[1] = Ïƒ(z[1])
z[2] = MMW [2],b[2](a[1])
a[2] = Ïƒ(z[2])
...
z[r] = MMW [r],b[r](a[râˆ’1])
J = â„“logistic(z[r], y) . (7.72)
We apply the backward function sequentially in a backward order. First, we
have that
âˆ‚J
âˆ‚z [r] = B[â„“logistic, z[r]]
(âˆ‚J
âˆ‚J
)
= B[â„“logistic, z[r]](1) . (7.73)
Then, we iteratively compute âˆ‚J
âˆ‚a[i] and âˆ‚J
âˆ‚z [i] â€™s by repeatedly invoking the chain
rule (equation (7.58)),
âˆ‚J
âˆ‚a[râˆ’1] = B[MM, a[râˆ’1]]
( âˆ‚J
âˆ‚z [r]
)
âˆ‚J
âˆ‚z [râˆ’1] = B[Ïƒ, z[râˆ’1]]
( âˆ‚J
âˆ‚a[râˆ’1]
)
...
âˆ‚J
âˆ‚z [1] = B[Ïƒ, z[1]]
( âˆ‚J
âˆ‚a[1]
)
. (7.74)

108
Numerically, we compute these quantities by repeatedly invoking equa-
tions (7.69) and (7.63) with diï¬€erent choices of variables.
We note that the intermediate values of a[i] and z[i] are used in the back-
propagation (equation (7.74)), and therefore these values need to be stored
in the memory after the forward pass.
Next, we compute the gradient of the parameters by invoking equa-
tions (7.65) and (7.66),
âˆ‚J
âˆ‚W [r] = B[MM, W [r]]
( âˆ‚J
âˆ‚z [r]
)
âˆ‚J
âˆ‚b[r] = B[MM, b[r]]
( âˆ‚J
âˆ‚z [r]
)
...
âˆ‚J
âˆ‚W [1] = B[MM, W [1]]
( âˆ‚J
âˆ‚z [1]
)
âˆ‚J
âˆ‚b[1] = B[MM, b[1]]
( âˆ‚J
âˆ‚z [1]
)
. (7.75)
We also note that the block of computations in equations (7.75) can be
interleaved with the block of computation in equations (7.74) because the
âˆ‚J
âˆ‚W [i] and âˆ‚J
âˆ‚b[i] can be computed as soon as âˆ‚J
âˆ‚z [i] is computed.
Putting all of these together, and explicitly invoking the equa-
tions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).

109
Algorithm 3 Back-propagation for multi-layer neural networks.
1: Forward pass. Compute and store the values of a[k]â€™s, z[k]â€™s, and J
using the equations (7.72).
2: Backward pass. Compute the gradient of loss J with respect to z[r]:
âˆ‚J
âˆ‚z [r] = B[â„“logistic, z[r]](1) =
(
1/(1 + exp(âˆ’z[r])) âˆ’ y
)
. (7.76)
3: for k = r âˆ’ 1 to 0 do
4: Compute the gradient with respect to parameters W [k+1] and b[k+1].
âˆ‚J
âˆ‚W [k+1] = B[MM, W [k+1]]
( âˆ‚J
âˆ‚z [k+1]
)
= âˆ‚J
âˆ‚z [k+1] a[k]âŠ¤
. (7.77)
âˆ‚J
âˆ‚b[k+1] = B[MM, b[k+1]]
( âˆ‚J
âˆ‚z [k+1]
)
= âˆ‚J
âˆ‚z [k+1] . (7.78)
5: When k â‰¥ 1, compute the gradient with respect to z[k] and a[k].
âˆ‚J
âˆ‚a[k] = B[Ïƒ, a[k]]
( âˆ‚J
âˆ‚z [k+1]
)
= W [k+1]âŠ¤ âˆ‚J
âˆ‚z [k+1] . (7.79)
âˆ‚J
âˆ‚z [k] = B[Ïƒ, z[k]]
( âˆ‚J
âˆ‚a[k]
)
= Ïƒâ€²(z[k]) âŠ™ âˆ‚J
âˆ‚a[k] . (7.80)
7.5 Vectorization over training examples
As we discussed in Section 7.1, in the implementation of neural networks,
we will leverage the parallelism across the multiple examples. This means
that we will need to write the forward pass (the evaluation of the outputs)
of the neural network and the backward pass (backpropagation) for multiple

110
training examples in matrix notation.
The basic idea. The basic idea is simple. Suppose you have a training
set with three examples x(1), x(2), x(3). The ï¬rst-layer activations for each
example are as follows:
z[1](1) = W [1]x(1) + b[1]
z[1](2) = W [1]x(2) + b[1]
z[1](3) = W [1]x(3) + b[1]
Note the diï¬€erence between square brackets [Â·], which refer to the layer num-
ber, and parenthesis ( Â·), which refer to the training example number. In-
tuitively, one would implement this using a for loop. It turns out, we can
vectorize these operations as well. First, deï¬ne:
X =
ï£®
ï£°
| | |
x(1) x(2) x(3)
| | |
ï£¹
ï£» âˆˆ RdÃ—3 (7.81)
Note that we are stacking training examples in columns and not rows. We
can then combine this into a single uniï¬ed formulation:
Z[1] =
ï£®
ï£°
| | |
z[1](1) z[1](2) z[1](3)
| | |
ï£¹
ï£» = W [1]X + b[1] (7.82)
You may notice that we are attempting to add b[1] âˆˆ R4Ã—1 to W [1]X âˆˆ
R4Ã—3. Strictly following the rules of linear algebra, this is not allowed. In
practice however, this addition is performed using broadcasting. We create
an intermediate Ëœb[1] âˆˆ R4Ã—3:
Ëœb[1] =
ï£®
ï£°
| | |
b[1] b[1] b[1]
| | |
ï£¹
ï£» (7.83)
We can then perform the computation: Z[1] = W [1]X + Ëœb[1]. Often times, it
is not necessary to explicitly construct Ëœb[1]. By inspecting the dimensions in
(7.82), you can assume b[1] âˆˆ R4Ã—1 is correctly broadcast to W [1]X âˆˆ R4Ã—3.
The matricization approach as above can easily generalize to multiple
layers, with one subtlety though, as discussed below.