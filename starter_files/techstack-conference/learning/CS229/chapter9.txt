134
x
x1
2
/0 /1
/0 /1
/0 /1
x
x1
2
In order words, under the deﬁnition of the VC dimension, in order to
prove that VC(H) is at least D, we need to show only that there’s at least
one set of size D that H can shatter.
The following theorem, due to Vapnik, can then be shown. (This is, many
would argue, the most important theorem in all of learning theory.)
Theorem. Let H be given, and let D = VC(H). Then with probability at
least 1 − δ, we have that for all h ∈ H,
|ε(h) − ˆε(h)| ≤ O
(√
D
n log n
D + 1
n log 1
δ
)
.
Thus, with probability at least 1 − δ, we also have that:
ε(ˆh) ≤ ε(h∗) + O
(√
D
n log n
D + 1
n log 1
δ
)
.
In other words, if a hypothesis class has ﬁnite VC dimension, then uniform
convergence occurs as n becomes large. As before, this allows us to give a
bound on ε(h) in terms of ε(h∗). We also have the following corollary:
Corollary. For |ε(h) − ˆε(h)| ≤ γ to hold for all h ∈ H (and hence ε(ˆh) ≤
ε(h∗) + 2γ) with probability at least 1 − δ, it suﬃces that n = Oγ,δ(D).
In other words, the number of training examples needed to learn “well”
using H is linear in the VC dimension of H. It turns out that, for “most”
hypothesis classes, the VC dimension (assuming a “reasonable” parameter-
ization) is also roughly linear in the number of parameters. Putting these
together, we conclude that for a given hypothesis class H (and for an algo-
rithm that tries to minimize training error), the number of training examples
needed to achieve generalization error close to that of the optimal classiﬁer
is usually roughly linear in the number of parameters of H.

Chapter 9
Regularization and model
selection
9.1 Regularization
Recall that as discussed in Section 8.1, overftting is typically a result of using
too complex models, and we need to choose a proper model complexity to
achieve the optimal bias-variance tradeoﬀ. When the model complexity is
measured by the number of parameters, we can vary the size of the model
(e.g., the width of a neural net). However, the correct, informative complex-
ity measure of the models can be a function of the parameters (e.g., ℓ2 norm
of the parameters), which may not necessarily depend on the number of pa-
rameters. In such cases, we will use regularization, an important technique
in machine learning, control the model complexity and prevent overﬁtting.
Regularization typically involves adding an additional term, called a reg-
ularizer and denoted by R(θ) here, to the training loss/cost function:
Jλ(θ) = J(θ) + λR(θ) (9.1)
Here Jλ is often called the regularized loss, and λ ≥ 0 is called the regular-
ization parameter. The regularizer R(θ) is a nonnegative function (in almost
all cases). In classical methods, R(θ) is purely a function of the parameter θ,
but some modern approach allows R(θ) to depend on the training dataset. 1
The regularizer R(θ) is typically chosen to be some measure of the com-
plexity of the model θ. Thus, when using the regularized loss, we aim to
ﬁnd a model that both ﬁt the data (a small loss J(θ)) and have a small
1Here our notations generally omit the dependency on the training dataset for
simplicity—we writeJ(θ) even though it obviously needs to depend on the training dataset.
135

136
model complexity (a small R(θ)). The balance between the two objectives is
controlled by the regularization parameter λ. When λ = 0, the regularized
loss is equivalent to the original loss. When λ is a suﬃciently small positive
number, minimizing the regularized loss is eﬀectively minimizing the original
loss with the regularizer as the tie-breaker. When the regularizer is extremely
large, then the original loss is not eﬀective (and likely the model will have a
large bias.)
The most commonly used regularization is perhaps ℓ2 regularization,
where R(θ) = 1
2 ∥θ∥2
2. It encourages the optimizer to ﬁnd a model with
small ℓ2 norm. In deep learning, it’s oftentimes referred to as weight de-
cay, because gradient descent with learning rate η on the regularized loss
Rλ(θ) is equivalent to shrinking/decaying θ by a scalar factor of 1 − ηλ and
then applying the standard gradient
θ ← θ − η∇Jλ(θ) = θ − ηλθ − η∇J(θ)
= (1 − λη)θ  
decaying weights
−η∇J(θ) (9.2)
Besides encouraging simpler models, regularization can also impose in-
ductive biases or structures on the model parameters. For example, suppose
we had a prior belief that the number of non-zeros in the ground-truth model
parameters is small,2—which is oftentimes called sparsity of the model—, we
can impose a regularization on the number of non-zeros in θ, denoted by
∥θ∥0, to leverage such a prior belief. Imposing additional structure of the
parameters narrows our search space and makes the complexity of the model
family smaller,—e.g., the family of sparse models can be thought of as having
lower complexity than the family of all models—, and thus tends to lead to a
better generalization. On the other hand, imposing additional structure may
risk increasing the bias. For example, if we regularize the sparsity strongly
but no sparse models can predict the label accurately, we will suﬀer from
large bias (analogously to the situation when we use linear models to learn
data than can only be represented by quadratic functions in Section 8.1.)
The sparsity of the parameters is not a continuous function of the param-
eters, and thus we cannot optimize it with (stochastic) gradient descent. A
common relaxation is to use R(θ) = ∥θ∥1 as a continuous surrogate. 3
2For linear models, this means the model just uses a few coordinates of the inputs to
make an accurate prediction.
3There has been a rich line of theoretical work that explains why ∥θ∥1 is a good sur-
rogate for encouraging sparsity, but it’s beyond the scope of this course. An intuition is:
assuming the parameter is on the unit sphere, the parameter with smallest ℓ1 norm also

137
The R(θ) = ∥θ∥1 (also called LASSO) and R(θ) = 1
2 ∥θ∥2
2 are perhaps
among the most commonly used regularizers for linear models. Other norm
and powers of norms are sometimes also used. The ℓ2 norm regularization is
much more commonly used with kernel methods because ℓ1 regularization is
typically not compatible with the kernel trick (the optimal solution cannot
be written as functions of inner products of features.)
In deep learning, the most commonly used regularizer is ℓ2 regularization
or weight decay. Other common ones include dropout, data augmentation,
regularizing the spectral norm of the weight matrices, and regularizing the
Lipschitzness of the model, etc. Regularization in deep learning is an ac-
tive research area, and it’s known that there is another implicit source of
regularization, as discussed in the next section.
9.2 Implicit regularization eﬀect (optional
reading)
The implicit regularization eﬀect of optimizers, or implicit bias or algorithmic
regularization, is a new concept/phenomenon observed in the deep learning
era. It largely refers to that the optimizers can implicitly impose structures
on parameters beyond what has been imposed by the regularized loss.
In most classical settings, the loss or regularized loss has a unique global
minimum, and thus any reasonable optimizer should converge to that global
minimum and cannot impose any additional preferences. However, in deep
learning, oftentimes the loss or regularized loss has more than one (approx-
imate) global minima, and diﬀerence optimizers may converge to diﬀerent
global minima. Though these global minima have the same or similar train-
ing losses, they may be of diﬀerent nature and have dramatically diﬀerent
generalization performance. See Figures 9.1 and 9.2 and its caption for an
illustration and some experiment results. For example, it’s possible that one
global minimum gives a much more Lipschitz or sparse model than others
and thus has a better test error. It turns out that many commonly-used op-
timizers (or their components) prefer or bias towards ﬁnding global minima
of certain properties, leading to a better test performance.
happen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and
ℓ1 norm gives the same extremal points to some extent.

138
θ
loss
Figure 9.1: An Illustration that diﬀerent global minima of the training loss
can have diﬀerent test performance.
Figure 9.2: Left: Performance of neural networks trained by two diﬀerent
learning rates schedules on the CIFAR-10 dataset. Although both exper-
iments used exactly the same regularized losses and the optimizers ﬁt the
training data perfectly, the models’ generalization performance diﬀer much.
Right: On a diﬀerent synthetic dataset, optimizers with diﬀerent initializa-
tions have the same training error but diﬀerent generalization performance. 4
In summary, the takehome message here is that the choice of optimizer
does not only aﬀect minimizing the training loss, but also imposes implicit
regularization and aﬀects the generalization of the model. Even if your cur-
rent optimizer already converges to a small training error perfectly, you may
still need to tune your optimizer for a better generalization, .
4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]

139
One may wonder which components of the optimizers bias towards what
type of global minima and what type of global minima may generalize bet-
ter. These are open questions that researchers are actively investigating.
Empirical and theoretical research have oﬀered some clues and heuristics.
In many (but deﬁnitely far from all) situations, among those setting where
optimization can succeed in minimizing the training loss, the use of larger
initial learning rate, smaller initialization, smaller batch size, and momen-
tum appears to help with biasing towards more generalizable solutions. A
conjecture (that can be proven in certain simpliﬁed case) is that stochas-
ticity in the optimization process help the optimizer to ﬁnd ﬂatter global
minima (global minima where the curvature of the loss is small), and ﬂat
global minima tend to give more Lipschitz models and better generalization.
Characterizing the implicit regularization eﬀect formally is still a challenging
open research question.
9.3 Model selection via cross validation
Suppose we are trying select among several diﬀerent models for a learning
problem. For instance, we might be using a polynomial regression model
hθ(x) = g(θ0 + θ1x + θ2x2 + · · · + θkxk), and wish to decide if k should be
0, 1, . . . , or 10. How can we automatically select a model that represents
a good tradeoﬀ between the twin evils of bias and variance 5? Alternatively,
suppose we want to automatically choose the bandwidth parameter τ for
locally weighted regression, or the parameter C for our ℓ1-regularized SVM.
How can we do that?
For the sake of concreteness, in these notes we assume we have some
ﬁnite set of models M = {M1, . . . , Md} that we’re trying to select among.
For instance, in our ﬁrst example above, the model Mi would be an i-th
degree polynomial regression model. (The generalization to inﬁnite M is
not hard.6) Alternatively, if we are trying to decide between using an SVM,
a neural network or logistic regression, then M may contain these models.
5Given that we said in the previous set of notes that bias and variance are two very
diﬀerent beasts, some readers may be wondering if we should be calling them “twin” evils
here. Perhaps it’d be better to think of them as non-identical twins. The phrase “the
fraternal twin evils of bias and variance” doesn’t have the same ring to it, though.
6If we are trying to choose from an inﬁnite set of models, say corresponding to the
possible values of the bandwidth τ ∈ R+, we may discretize τ and consider only a ﬁnite
number of possible values for it. More generally, most of the algorithms described here
can all be viewed as performing optimization search in the space of models, and we can
perform this search over inﬁnite model classes as well.

140
Cross validation. Lets suppose we are, as usual, given a training set S.
Given what we know about empirical risk minimization, here’s what might
initially seem like a algorithm, resulting from using empirical risk minimiza-
tion for model selection:
1. Train each model Mi on S, to get some hypothesis hi.
2. Pick the hypotheses with the smallest training error.
This algorithm does not work. Consider choosing the degree of a poly-
nomial. The higher the degree of the polynomial, the better it will ﬁt the
training set S, and thus the lower the training error. Hence, this method will
always select a high-variance, high-degree polynomial model, which we saw
previously is often poor choice.
Here’s an algorithm that works better. In hold-out cross validation
(also called simple cross validation), we do the following:
1. Randomly split S into Strain (say, 70% of the data) and Scv (the remain-
ing 30%). Here, Scv is called the hold-out cross validation set.
2. Train each model Mi on Strain only, to get some hypothesis hi.
3. Select and output the hypothesis hi that had the smallest error ˆεScv(hi)
on the hold out cross validation set. (Here ˆ εScv(h) denotes the average
error of h on the set of examples in Scv.) The error on the hold out
validation set is also referred to as the validation error.
By testing/validating on a set of examples Scv that the models were not
trained on, we obtain a better estimate of each hypothesis hi’s true general-
ization/test error. Thus, this approach is essentially picking the model with
the smallest estimated generalization/test error. The size of the validation
set depends on the total number of available examples. Usually, somewhere
between 1/4−1/3 of the data is used in the hold out cross validation set, and
30% is a typical choice. However, when the total dataset is huge, validation
set can be a smaller fraction of the total examples as long as the absolute
number of validation examples is decent. For example, for the ImageNet
dataset that has about 1M training images, the validation set is sometimes
set to be 50K images, which is only about 5% of the total examples.
Optionally, step 3 in the algorithm may also be replaced with selecting
the model Mi according to arg min i ˆεScv(hi), and then retraining Mi on the
entire training set S. (This is often a good idea, with one exception being
learning algorithms that are be very sensitive to perturbations of the initial

141
conditions and/or data. For these methods, Mi doing well on Strain does not
necessarily mean it will also do well on Scv, and it might be better to forgo
this retraining step.)
The disadvantage of using hold out cross validation is that it “wastes”
about 30% of the data. Even if we were to take the optional step of retraining
the model on the entire training set, it’s still as if we’re trying to ﬁnd a good
model for a learning problem in which we had 0.7n training examples, rather
than n training examples, since we’re testing models that were trained on
only 0.7n examples each time. While this is ﬁne if data is abundant and/or
cheap, in learning problems in which data is scarce (consider a problem with
n = 20, say), we’d like to do something better.
Here is a method, called k-fold cross validation , that holds out less
data each time:
1. Randomly split S into k disjoint subsets of m/k training examples each.
Lets call these subsets S1, . . . , Sk.
2. For each model Mi, we evaluate it as follows:
For j = 1, . . . , k
Train the model Mi on S1 ∪ · · · ∪Sj−1 ∪ Sj+1 ∪ · · ·Sk (i.e., train
on all the data except Sj) to get some hypothesis hij.
Test the hypothesis hij on Sj, to get ˆεSj(hij).
The estimated generalization error of model Mi is then calculated
as the average of the ˆεSj(hij)’s (averaged over j).
3. Pick the model Mi with the lowest estimated generalization error, and
retrain that model on the entire training set S. The resulting hypothesis
is then output as our ﬁnal answer.
A typical choice for the number of folds to use here would be k = 10.
While the fraction of data held out each time is now 1 /k—much smaller
than before—this procedure may also be more computationally expensive
than hold-out cross validation, since we now need train to each model k
times.
While k = 10 is a commonly used choice, in problems in which data is
really scarce, sometimes we will use the extreme choice of k = m in order
to leave out as little data as possible each time. In this setting, we would
repeatedly train on all but one of the training examples in S, and test on that
held-out example. The resulting m = k errors are then averaged together to
obtain our estimate of the generalization error of a model. This method has

142
its own name; since we’re holding out one training example at a time, this
method is called leave-one-out cross validation.
Finally, even though we have described the diﬀerent versions of cross vali-
dation as methods for selecting a model, they can also be used more simply to
evaluate a single model or algorithm. For example, if you have implemented
some learning algorithm and want to estimate how well it performs for your
application (or if you have invented a novel learning algorithm and want to
report in a technical paper how well it performs on various test sets), cross
validation would give a reasonable way of doing so.
9.4 Bayesian statistics and regularization
In this section, we will talk about one more tool in our arsenal for our battle
against overﬁtting.
At the beginning of the quarter, we talked about parameter ﬁtting using
maximum likelihood estimation (MLE), and chose our parameters according
to
θMLE = arg max
θ
n∏
i=1
p(y(i)|x(i); θ).
Throughout our subsequent discussions, we viewed θ as an unknown param-
eter of the world. This view of the θ as being constant-valued but unknown
is taken in frequentist statistics. In the frequentist this view of the world, θ
is not random—it just happens to be unknown—and it’s our job to come up
with statistical procedures (such as maximum likelihood) to try to estimate
this parameter.
An alternative way to approach our parameter estimation problems is to
take the Bayesian view of the world, and think of θ as being a random
variable whose value is unknown. In this approach, we would specify a
prior distribution p(θ) on θ that expresses our “prior beliefs” about the
parameters. Given a training set S = {(x(i), y(i))}n
i=1, when we are asked to
make a prediction on a new value of x, we can then compute the posterior
distribution on the parameters
p(θ|S) = p(S|θ)p(θ)
p(S)
=
(∏n
i=1 p(y(i)|x(i), θ)
)
p(θ)∫
θ (∏n
i=1 p(y(i)|x(i), θ)p(θ)) dθ (9.3)
In the equation above, p(y(i)|x(i), θ) comes from whatever model you’re using