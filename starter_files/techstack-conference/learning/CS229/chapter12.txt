164
With this re-parameterization, we have that
Ez(i)∼Qi
[
log p(x(i), z(i); θ)
Qi(z(i))
]
(11.25)
= Eξ(i)∼N (0,1)
[
log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i); θ)
Qi(q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i))
]
It follows that
∇φEz(i)∼Qi
[
log p(x(i), z(i); θ)
Qi(z(i))
]
= ∇φEξ(i)∼N (0,1)
[
log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i); θ)
Qi(q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i))
]
= Eξ(i)∼N (0,1)
[
∇φ log p(x(i), q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i); θ)
Qi(q(x(i); φ) + v(x(i); ψ) ⊙ ξ(i))
]
We can now sample multiple copies of ξ(i)’s to estimate the the expecta-
tion in the RHS of the equation above. 9 We can estimate the gradient with
respect to ψ similarly, and with these, we can implement the gradient ascent
algorithm to optimize the ELBO over φ, ψ, θ.
There are not many high-dimensional distributions with analytically com-
putable density function are known to be re-parameterizable. We refer to
Kingma and Welling [2013] for a few other choices that can replace Gaussian
distribution.
9Empirically people sometimes just use one sample to estimate it for maximum com-
putational eﬃciency.

Chapter 12
Principal components analysis
In this set of notes, we will develop a method, Principal Components Analysis
(PCA), that tries to identify the subspace in which the data approximately
lies. PCA is computationally eﬃcient: it will require only an eigenvector
calculation (easily done with the eig function in Matlab).
Suppose we are given a dataset {x(i); i = 1, . . . , n} of attributes of n dif-
ferent types of automobiles, such as their maximum speed, turn radius, and
so on. Let x(i) ∈ Rd for each i (d ≪ n). But unknown to us, two diﬀerent
attributes—some xi and xj—respectively give a car’s maximum speed mea-
sured in miles per hour, and the maximum speed measured in kilometers per
hour. These two attributes are therefore almost linearly dependent, up to
only small diﬀerences introduced by rounding oﬀ to the nearest mph or kph.
Thus, the data really lies approximately on an n − 1 dimensional subspace.
How can we automatically detect, and perhaps remove, this redundancy?
For a less contrived example, consider a dataset resulting from a survey of
pilots for radio-controlled helicopters, where x(i)
1 is a measure of the piloting
skill of pilot i, and x(i)
2 captures how much he/she enjoys ﬂying. Because
RC helicopters are very diﬃcult to ﬂy, only the most committed students,
ones that truly enjoy ﬂying, become good pilots. So, the two attributes
x1 and x2 are strongly correlated. Indeed, we might posit that that the
data actually likes along some diagonal axis (the u1 direction) capturing the
intrinsic piloting “karma” of a person, with only a small amount of noise
lying oﬀ this axis. (See ﬁgure.) How can we automatically compute this u1
direction?
165

166
x1
x2 (enjoyment)
(skill)
1
u
u
2
We will shortly develop the PCA algorithm. But prior to running PCA
per se, typically we ﬁrst preprocess the data by normalizing each feature
to have mean 0 and variance 1. We do this by subtracting the mean and
dividing by the empirical standard deviation:
x(i)
j ← x(i)
j − µj
σj
where µj = 1
n
∑n
i=1 x(i)
j and σ2
j = 1
n
∑n
i=1(x(i)
j − µj)2 are the mean variance of
feature j, respectively.
Subtracting µj zeros out the mean and may be omitted for data known
to have zero mean (for instance, time series corresponding to speech or other
acoustic signals). Dividing by the standard deviation σj rescales each coor-
dinate to have unit variance, which ensures that diﬀerent attributes are all
treated on the same “scale.” For instance, if x1 was cars’ maximum speed in
mph (taking values in the high tens or low hundreds) and x2 were the num-
ber of seats (taking values around 2-4), then this renormalization rescales
the diﬀerent attributes to make them more comparable. This rescaling may
be omitted if we had a priori knowledge that the diﬀerent attributes are all
on the same scale. One example of this is if each data point represented a
grayscale image, and each x(i)
j took a value in {0, 1, . . . ,255} corresponding
to the intensity value of pixel j in image i.
Now, having normalized our data, how do we compute the “major axis
of variation” u—that is, the direction on which the data approximately lies?
One way is to pose this problem as ﬁnding the unit vector u so that when

167
the data is projected onto the direction corresponding to u, the variance of
the projected data is maximized. Intuitively, the data starts oﬀ with some
amount of variance/information in it. We would like to choose a direction u
so that if we were to approximate the data as lying in the direction/subspace
corresponding to u, as much as possible of this variance is still retained.
Consider the following dataset, on which we have already carried out the
normalization steps:
Now, suppose we pick u to correspond the the direction shown in the
ﬁgure below. The circles denote the projections of the original data onto this
line.

168
/0
/0
/1
/1
/0/0
/0/0
/1/1
/1/1
/0 /1
/0/0
/0/0
/1/1
/1/1
/0 /1/0/0
/0/0
/1/1
/1/1
/0/0
/0/0
/0/0
/0/0
/0/0
/0/0
/0/0
/1/1
/1/1
/1/1
/1/1
/1/1
/1/1
/1/1
/0/0
/0/0
/0/0
/0/0
/1/1
/1/1
/1/1
/1/1
/0/0/0
/0/0/0
/0/0/0
/0/0/0
/0/0/0
/0/0/0
/0/0/0
/1/1/1
/1/1/1
/1/1/1
/1/1/1
/1/1/1
/1/1/1
/1/1/1
/0/0
/0/0
/0/0
/0/0
/1/1
/1/1
/1/1
/1/1
We see that the projected data still has a fairly large variance, and the
points tend to be far from zero. In contrast, suppose had instead picked the
following direction:
/0/0 /1/1
/0/0
/0/0
/1/1
/1/1
/0 /1
/0/0
/0/0
/1/1
/1/1
/0
/0
/1
/1
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0/0/0/0
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1/1/1/1
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/0/0/0/0/0/0
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/1/1/1/1/1/1
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/0/0/0/0/0/0/0/0
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/1/1/1/1/1/1/1/1
/0/0/0/0/0
/0/0/0/0/0
/0/0/0/0/0
/0/0/0/0/0
/0/0/0/0/0
/0/0/0/0/0
/0/0/0/0/0
/0/0/0/0/0
/1/1/1/1/1
/1/1/1/1/1
/1/1/1/1/1
/1/1/1/1/1
/1/1/1/1/1
/1/1/1/1/1
/1/1/1/1/1
/1/1/1/1/1
Here, the projections have a signiﬁcantly smaller variance, and are much
closer to the origin.
We would like to automatically select the direction u corresponding to
the ﬁrst of the two ﬁgures shown above. To formalize this, note that given a

169
unit vector u and a point x, the length of the projection of x onto u is given
by xT u. I.e., if x(i) is a point in our dataset (one of the crosses in the plot),
then its projection onto u (the corresponding circle in the ﬁgure) is distance
xT u from the origin. Hence, to maximize the variance of the projections, we
would like to choose a unit-length u so as to maximize:
1
n
n∑
i=1
(x(i)T
u)2 = 1
n
n∑
i=1
uT x(i)x(i)T
u
= uT
(
1
n
n∑
i=1
x(i)x(i)T
)
u.
We easily recognize that the maximizing this subject to ∥u∥2 = 1 gives the
principal eigenvector of Σ = 1
n
∑n
i=1 x(i)x(i)T
, which is just the empirical
covariance matrix of the data (assuming it has zero mean). 1
To summarize, we have found that if we wish to ﬁnd a 1-dimensional
subspace with with to approximate the data, we should choose u to be the
principal eigenvector of Σ. More generally, if we wish to project our data
into a k-dimensional subspace (k < d ), we should choose u1, . . . , uk to be the
top k eigenvectors of Σ. The ui’s now form a new, orthogonal basis for the
data.2
Then, to represent x(i) in this basis, we need only compute the corre-
sponding vector
y(i) =


uT
1 x(i)
uT
2 x(i)
...
uT
k x(i)

 ∈ Rk.
Thus, whereas x(i) ∈ Rd, the vector y(i) now gives a lower, k-dimensional,
approximation/representation for x(i). PCA is therefore also referred to as
a dimensionality reduction algorithm. The vectors u1, . . . , uk are called
the ﬁrst k principal components of the data.
Remark. Although we have shown it formally only for the case of k = 1,
using well-known properties of eigenvectors it is straightforward to show that
1If you haven’t seen this before, try using the method of Lagrange multipliers to max-
imizeuT Σu subject to that uTu = 1. You should be able to show that Σ u =λu, for some
λ, which implies u is an eigenvector of Σ, with eigenvalue λ.
2Because Σ is symmetric, the ui’s will (or always can be chosen to be) orthogonal to
each other.