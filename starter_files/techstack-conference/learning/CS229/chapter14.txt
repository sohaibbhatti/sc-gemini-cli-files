176
where Î± is the learning rate.
After the algorithm converges, we then compute s(i) = W x(i) to recover
the original sources.
Remark. When writing down the likelihood of the data, we implicitly as-
sumed that the x(i)â€™s were independent of each other (for diï¬€erent values
of i; note this issue is diï¬€erent from whether the diï¬€erent coordinates of
x(i) are independent), so that the likelihood of the training set was given
by âˆ
i p(x(i); W ). This assumption is clearly incorrect for speech data and
other time series where the x(i)â€™s are dependent, but it can be shown that
having correlated training examples will not hurt the performance of the al-
gorithm if we have suï¬ƒcient data. However, for problems where successive
training examples are correlated, when implementing stochastic gradient as-
cent, it sometimes helps accelerate convergence if we visit training examples
in a randomly permuted order. (I.e., run stochastic gradient ascent on a
randomly shuï¬„ed copy of the training set.)

Chapter 14
Self-supervised learning and
foundation models
Despite its huge success, supervised learning with neural networks typically
relies on the availability of a labeled dataset of decent size, which is some-
times costly to collect. Recently, AI and machine learning are undergoing a
paradigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and
GPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and
are adaptable to a wide range of downstream tasks. These models, called
foundation models by Bommasani et al. [2021], oftentimes leverage massive
unlabeled data so that much fewer labeled data in the downstream tasks are
needed. Moreover, though foundation models are based on standard deep
learning and transfer learning, their scale results in new emergent capabil-
ities. These models are typically (pre-)trained by self-supervised learning
methods where the supervisions/labels come from parts of the inputs.
This chapter will introduce the paradigm of foundation models and basic
related concepts.
14.1 Pretraining and adaptation
The foundation models paradigm consists of two phases: pretraining (or sim-
ply training) and adaptation. We ï¬rst pretrain a large model on a massive
unlabeled dataset (e.g., billions of unlabeled images). 1 Then, we adapt the
pretrained model to a downstream task (e.g., detecting cancer from scan im-
ages). These downstream tasks are often prediction tasks with limited or
1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-
geNet dataset).
177

178
even no labeled data. The intuition is that the pretrained models learn good
representations that capture intrinsic semantic structure/ information about
the data, and the adaptation phase customizes the model to a particular
downstream task by, e.g., retrieving the information speciï¬c to it. For ex-
ample, a model pretrained on massive unlabeled image data may learn good
general visual representations/features, and we adapt the representations to
solve biomedical imagining tasks.
We formalize the two phases below.
Pretraining. Suppose we have an unlabeled pretraining dataset
{x(1), x(2) Â· Â· Â· , x(n)} that consists of n examples in Rd. Let Ï†Î¸ be a model that
is parameterized by Î¸ and maps the input x to some m-dimensional represen-
tation Ï†Î¸(x). (People also call Ï†Î¸(x) âˆˆ Rm the embedding or features of the
example x.) We pretrain the model Î¸ with a pretraining loss, which is often
an average of loss functions on all the examples:Lpre(Î¸) = 1
n
âˆ‘n
i=1 â„“pre(Î¸, x(i)).
Here â„“pre is a so-called self-supervised loss on a single datapoint x(i), because
as shown later, e.g., in Section 14.3, the â€œsupervisionâ€ comes from the data
point x(i) itself. It is also possible that the pretraining loss is not a sum
of losses on individual examples. We will discuss two pretraining losses in
Section 14.2 and Section 14.3.
We use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,
2014]) to minimize Lpre(Î¸). We denote the obtained pretrained model by Ë†Î¸.
Adaptation. For a downstream task, we usually have a labeled dataset
{(x(1)
task, y(1)
task), Â· Â· Â· , (x(ntask)
task , y(ntask)
task )} with ntask examples. The setting when
ntask = 0 is called zero-shot learningâ€”the downstream task doesnâ€™t have any
labeled examples. When ntask is relatively small (say, between 1 and 50), the
setting is called few-shot learning. Itâ€™s also pretty common to have a larger
ntask on the order of ranging from hundreds to tens of thousands.
An adaptation algorithm generally takes in a downstream dataset and the
pretrained model Ë†Î¸, and outputs a variant of Ë†Î¸ that solves the downstream
task. We will discuss below two popular and general adaptation methods,
linear probe and ï¬netuning. In addition, two other methods speciï¬c to lan-
guage problems are introduced in 14.3.2.
The linear probe approach uses a linear head on top of the representation
to predict the downstream labels. Mathematically, the adapted model out-
puts wâŠ¤Ï†Ë†Î¸(x), where w âˆˆ Rm is a parameter to be learned, and Ë†Î¸ is exactly
the pretrained model (ï¬xed). We can use SGD (or other optimizers) to train

179
w on the downstream task loss to predict the task label
min
wâˆˆRm
1
ntask
ntaskâˆ‘
i=1
â„“task(y(i)
task, wâŠ¤Ï†Ë†Î¸(x(i)
task)) (14.1)
E.g., if the downstream task is a regression problem, we will have
â„“task(ytask, wâŠ¤Ï†Ë†Î¸(xtask)) = (ytask âˆ’ wâŠ¤Ï†Ë†Î¸(xtask))2.
The ï¬netuning algorithm uses a similar structure for the downstream
prediction model, but also further ï¬netunes the pretrained model (instead
of keeping it ï¬xed). Concretely, the prediction model is wâŠ¤Ï†Î¸(x) with pa-
rameters w and Î¸. We optimize both w and Î¸ to ï¬t the downstream data,
but initialize Î¸ with the pretrained model Ë†Î¸. The linear head w is usually
initialized randomly.
minimize
w,Î¸
1
ntask
ntaskâˆ‘
i=1
â„“task(y(i)
task, wâŠ¤Ï†Î¸(x(i)
task)) (14.2)
with initialization w â† random vector (14.3)
Î¸ â† Ë†Î¸ (14.4)
Various other adaptation methods exists and are sometimes specialized
to the particular pretraining methods. We will discuss one of them in Sec-
tion 14.3.2.
14.2 Pretraining methods in computer vision
This section introduces two concrete pretraining methods for computer vi-
sion: supervised pretraining and contrastive learning.
Supervised pretraining. Here, the pretraining dataset is a large-scale
labeled dataset (e.g., ImageNet), and the pretrained models are simply a
neural network trained with vanilla supervised learning (with the last layer
being removed). Concretely, suppose we write the learned neural network as
U Ï†Ë†Î¸(x), where U is the last (fully-connected) layer parameters, Ë†Î¸ corresponds
to the parameters of all the other layers, and Ï†Ë†Î¸(x) are the penultimate
activations layer (which serves as the representation). We simply discard U
and use Ï†Ë†Î¸(x) as the pretrained model.
Contrastive learning. Contrastive learning is a self-supervised pretraining
method that uses only unlabeled data. The main intuition is that a good
representation function Ï†Î¸(Â·) should map semantically similar images to sim-
ilar representations, and that random pair of images should generally have

180
distinct representations. E.g., we may want to map images of two huskies to
similar representations, but a husky and an elephant should have diï¬€erent
representations. One deï¬nition of similarity is that images from the same
class are similar. Using this deï¬nition will result in the so-called supervised
contrastive algorithms that work well when labeled pretraining datasets are
available.
Without labeled data, we can use data augmentation to generate a pair
of â€œsimilarâ€ augmented images given an original image x. Data augmenta-
tion typically means that we apply random cropping, ï¬‚ipping, and/or color
transformation on the original image x to generate a variant. We can take
two random augmentations, denoted by Ë†x and Ëœx, of the same original image
x, and call them a positive pair. We observe that positive pairs of images
are often semantically related because they are augmentations of the same
image. We will design a loss function for Î¸ such that the representations of
a positive pair, Ï†Î¸(Ë†x), Ï†Î¸(Ëœx), as close to each other as possible.
On the other hand, we can also take another random image z from the
pretraining dataset and generate an augmentation Ë†z from z. Note that (Ë†x, Ë†z)
are from diï¬€erent images; therefore, with a good chance, they are not seman-
tically related. We call (Ë†x, Ë†z) a negative or random pair. 2 We will design a
loss to push the representation of random pairs, Ï†Î¸(Ë†x), Ï†Î¸(Ë†z), far away from
each other.
There are many recent algorithms based on the contrastive learning prin-
ciple, and here we introduce SIMCLR [Chen et al., 2020] as an concrete
example. The loss function is deï¬ned on a batch of examples ( x1, Â· Â· Â· , x(B))
with batch size B. The algorithm computes two random augmentations for
each example x(i) in the batch, denoted by Ë† x(i) and Ëœx(i). As a result, we
have the augmented batch of 2 B examples: Ë†x1, Â· Â· Â· , Ë†x(B), Ëœx1, Â· Â· Â· , Ëœx(B). The
SIMCLR loss is deï¬ned as 3
Lpre(Î¸) = âˆ’
Bâˆ‘
i=1
log exp
(
Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(i))
)
exp (Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(i))) + âˆ‘
jÌ¸=i exp (Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(j))) .
The intuition is as follows. The loss is increasing in Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(j)), and
thus minimizing the loss encourages Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(j)) to be small, making
Ï†Î¸(Ë†x(i)) far away from Ï†Î¸(Ëœx(j)). On the other hand, the loss is decreasing in
2Random pair may be a more accurate term because itâ€™s still possible (though not
likely) that x and z are semantically related, so are Ë†x and Ë†z. But in the literature, the
term negative pair seems to be also common.
3This is a variant and simpliï¬cation of the original loss that does not change the essence
(but may change the eï¬ƒciency slightly).

181
Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(i)), and thus minimizing the loss encourages Ï†Î¸(Ë†x(i))âŠ¤Ï†Î¸(Ëœx(i))
to be large, resulting in Ï†Î¸(Ë†x(i)) and Ï†Î¸(Ëœx(i)) to be close. 4
14.3 Pretrained large language models
Natural language processing is another area where pretraining models are
particularly successful. In language problems, an example typically corre-
sponds to a document or generally a sequence/trunk of words, 5 denoted
by x = ( x1, Â· Â· Â· , xT ) where T is the length of the document/sequence,
xi âˆˆ {1, Â· Â· Â· , V } are words in the document, and V is the vocabulary size. 6
A language model is a probabilistic model representing the probability of
a document, denoted by p(x1, Â· Â· Â· , xT ). This probability distribution is very
complex because its support size is V T â€” exponential in the length of the
document. Instead of modeling the distribution of a document itself, we can
apply the chain rule of conditional probability to decompose it as follows:
p(x1, Â· Â· Â· , xT ) = p(x1)p(x2|x1) Â· Â· Â· p(xT |x1, Â· Â· Â· , xTâˆ’1). (14.5)
Now the support size of each of the conditional probability p(xt|x1, Â· Â· Â· , xtâˆ’1)
is V .
We will model the conditional probability p(xt|x1, Â· Â· Â· , xtâˆ’1) as a function
of x1, . . . , xtâˆ’1 parameterized by some parameter Î¸.
A parameterized model takes in numerical inputs and therefore we ï¬rst
introduce embeddings or representations fo the words. Let ei âˆˆ Rd be the
embedding of the word i âˆˆ { 1, 2, Â· Â· Â· , V }. We call [ e1, Â· Â· Â· , eV ] âˆˆ RdÃ—V the
embedding matrix.
The most commonly used model is Transformer [Vaswani et al., 2017]. In
this subsection, we will introduce the input-output interface of a Transformer,
but treat the intermediate computation in the Transformer as a blackbox. We
refer the students to the transformer paper or more advanced courses for more
details. As shown in Figure 14.1, given a document ( x1, Â· Â· Â· , xT ), we ï¬rst
translate the sequence of discrete variables into a sequence of corresponding
4To see this, you can verify that the functionâˆ’ log p
p+q is decreasing inp, and increasing
in q when p,q >0.
5In the practical implementations, typically all the data are concatenated into a single
sequence in some order, and each example typically corresponds a sub-sequence of consec-
utive words which may corresponds to a subset of a document or may span across multiple
documents.
6Technically, words may be decomposed into tokens which could be words or sub-words
(combinations of letters), but this note omits this technicality. In fact most commons words
are a single token themselves.

182
word embeddings ( ex1, Â· Â· Â· , exT ). We also introduce a ï¬xed special token
x0 = âŠ¥ in the vocabulary with corresponding embedding ex0 to mark the
beginning of a document. Then, the word embeddings are passed into a
Transformer model, which takes in a sequence of vectors ( ex0, ex1, Â· Â· Â· , exT )
and outputs a sequence of vectors ( u1, u2, Â· Â· Â· , uT +1), where ut âˆˆ RV will be
interpreted as the logits for the probability distribution of the next word.
Here we use the autoregressive version of the Transformers which by design
ensures ut only depends on x1, Â· Â· Â· , xtâˆ’1 (note that this property does not
hold in masked language models [Devlin et al., 2019] where the losses are
also diï¬€erent.) We view the whole mapping from xâ€™s to uâ€™s a blackbox in
this subsection and call it a Transformer, denoted it by fÎ¸, where Î¸ include
both the parameters in the Transformer and the input embeddings. We write
ut = fÎ¸(x0, x1, . . . , xtâˆ’1) where fÎ¸ denotes the mapping from the input to the
outputs.
ğ‘¥!ğ‘¥" ğ‘¥#ğ‘’$!ğ‘’$" ğ‘’$#â€¦
Transformer ğ‘“%(ğ‘¥)
ğ‘¥&ğ‘’$$
ğ‘¢"ğ‘¢' ğ‘¢#(!ğ‘¢! â€¦
Figure 14.1: The inputs and outputs of a Transformer model.
The conditional probabilityp(xt|x1, Â· Â· Â· , xtâˆ’1) is the softtmax of the logits:
ï£®
ï£¯ï£¯ï£¯ï£°
p(xt = 1|x1 Â· Â· Â· , xtâˆ’1)
p(xt = 2|x1 Â· Â· Â· , xtâˆ’1)
...
p(xt = V |x1 Â· Â· Â· , xtâˆ’1)
ï£¹
ï£ºï£ºï£ºï£» = softmax(ut) âˆˆ RV (14.6)
= softmax(fÎ¸(x0, . . . , xtâˆ’1)) (14.7)
We train the Transformer parameter Î¸ by minimizing the negative log-
likelihood of seeing the data under the probabilistic model deï¬ned by Î¸,

183
which is the cross-entropy loss on the logitis.
loss(Î¸) = 1
T
Tâˆ‘
t=1
âˆ’ log(pÎ¸(xt|x1, . . . , xtâˆ’1)) (14.8)
= 1
T
Tâˆ‘
t=1
â„“ce(fÎ¸(x0, x1, Â· Â· Â· , xtâˆ’1), xt)
= 1
T
Tâˆ‘
t=1
âˆ’ log(softmax(fÎ¸(x0, x1, Â· Â· Â· , xtâˆ’1))xt) .
Autoregressive text decoding / generation. Given a autoregressive
Transformer, we can simply sample text from it sequentially. Given a pre-
ï¬x x1, . . . xt, we generate text completion xt+1, . . . xT sequentially using the
conditional distribution.
xt+1 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt)) (14.9)
xt+2 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt+1)) (14.10)
. . . (14.11)
xT âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xTâˆ’1)) . (14.12)
Note that each generated token is used as the input to the model when gen-
erating the following tokens. In practice, people often introduce a parameter
Ï„ > 0 named temperature to further adjust the entropy/sharpness of the
generated distribution,
xt+1 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt)/Ï„) (14.13)
xt+2 âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xt+1)/Ï„) (14.14)
. . . (14.15)
xT âˆ¼ softmax(fÎ¸(x0, x1, Â· Â· Â· , xTâˆ’1)/Ï„) . (14.16)
When Ï„ = 1, the text is sampled from the original conditional probability
deï¬ned by the model. With a decreasing Ï„, the generated text gradually
becomes more â€œdeterministicâ€. Ï„ â†’ 0 reduces to greedy decoding, where we
generate the most probable next token from the conditional probability.
14.3.1 Zero-shot learning and in-context learning
For language models, there are many ways to adapt a pretrained model to
downstream tasks. In this notes, we discuss three of them: ï¬netuning, zero-
shot learning, and in-context learning.

184
Finetuning is not very common for the autoregressive language models that
we introduced in Section 14.3 but much more common for other variants
such as masked language models which has similar input-output interfaces
but are pretrained diï¬€erently [Devlin et al., 2019]. The ï¬netuning method is
the same as introduced generally in Section 14.1â€”the only question is how
we deï¬ne the prediction task with an additional linear head. One option
is to treat cT +1 = Ï†Î¸(x1, Â· Â· Â· , xT ) as the representation and use wâŠ¤cT +1 =
wâŠ¤Ï†Î¸(x1, Â· Â· Â· , xT ) to predict task label. As described in Section 14.1, we
initialize Î¸ to the pretrained model Ë†Î¸ and then optimize both w and Î¸.
Zero-shot adaptation or zero-shot learning is the setting where there is no
input-output pairs from the downstream tasks. For language problems tasks,
typically the task is formatted as a question or a cloze test form via natural
language. For example, we can format an example as a question:
xtask = (xtask,1, Â· Â· Â· , xtask,T ) = â€œIs the speed of light a universal constant?â€
Then, we compute the most likely next word predicted by the lan-
guage model given this question, that is, computing argmax xT +1p(xT +1 |
xtask,1, Â· Â· Â· , xtask,T ). In this case, if the most likely next word xT +1 is â€œNoâ€,
then we solve the task. (The speed of light is only a constant in vacuum).
We note that there are many ways to decode the answer from the language
models, e.g., instead of computing the argmax, we may use the language
model to generate a few words word. It is an active research question to ï¬nd
the best way to utilize the language models.
In-context learning is mostly used for few-shot settings where we have a
few labeled examples (x(1)
task, y(1)
task), Â· Â· Â· , (x(ntask)
task , y(ntask)
task ). Given a test example
xtest, we construct a document ( x1, Â· Â· Â· , xT ), which is more commonly called
a â€œpromptâ€ in this context, by concatenating the labeled examples and the
text example in some format. For example, we may construct the prompt as
follows
x1, Â· Â· Â·,x T = â€œQ: 2 âˆ¼ 3 = ? x(1)
task
A: 5 y(1)
task
Q: 6 âˆ¼ 7 = ? x(2)
task
A: 13 y(2)
task
Â· Â· Â·
Q: 15 âˆ¼ 2 = ?â€ xtest

185
Then, we let the pretrained model generate the most likely xT +1, xT +2, Â· Â· Â· .
In this case, if the model can â€œlearnâ€ that the symbol âˆ¼ means addition from
the few examples, we will obtain the following which suggests the answer is
17.
xT +1, xT +2, Â· Â· Â· = â€œA: 17â€.
The area of foundation models is very new and quickly growing. The notes
here only attempt to introduce these models on a conceptual level with a
signiï¬cant amount of simpliï¬cation. We refer the readers to other materials,
e.g., Bommasani et al. [2021], for more details.

Part V
Reinforcement Learning and
Control
186